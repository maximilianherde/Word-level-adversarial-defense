{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ADV_TRAIN_USING_ADVDATA.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# MOUNT G_DRIVE\n",
        "from pathlib import Path\n",
        "from IPython import get_ipython\n",
        "on_colab = 'google.colab' in str(get_ipython())\n",
        "\n",
        "if on_colab:\n",
        "  from google.colab import drive\n",
        "  drive.mount(\"/content/gdrive\")\n",
        "\n",
        "PATH =  \"/content/gdrive/My Drive/DeepLearning/MODELS/\" if on_colab else \"./\"\n",
        "PATH_DATASET = \"/content/gdrive/My Drive/DeepLearning/DATASETS/\" if on_colab else \"./\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CeAOqnjy-4xl",
        "outputId": "81af882d-b457-45ff-bc33-882ab565ef56"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MODELS"
      ],
      "metadata": {
        "id": "CuzOajo4Lq0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Linear, Softmax, Conv2d, Dropout\n",
        "import torch.nn as nn\n",
        "from torch.nn import LSTM, GRU, Linear, Softmax\n",
        "from torchtext.vocab import GloVe\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torch.optim import Adam\n",
        "from tqdm import tqdm\n",
        "from torchtext.datasets import IMDB, AG_NEWS, YahooAnswers\n",
        "from torchtext.data import to_map_style_dataset\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "\n",
        "class BidirectionalLSTMClassifier(torch.nn.Module):\n",
        "    def __init__(self, num_classes, hidden_size, num_layers):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.LSTM = LSTM(50, hidden_size, num_layers=num_layers,\n",
        "                         batch_first=True, bidirectional=True)\n",
        "        self.linear = Linear(2 * hidden_size, num_classes)\n",
        "        self.softmax = Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, (h_n, _) = self.LSTM(x)\n",
        "        h_forward = h_n[2 * self.num_layers - 2]\n",
        "        h_backward = h_n[2 * self.num_layers - 1]\n",
        "        y = self.linear(torch.cat((h_forward, h_backward), 1))\n",
        "        return self.softmax(y)\n",
        "\n",
        "\n",
        "class CNNClassifier(torch.nn.Module):\n",
        "    def __init__(self, num_classes, in_channels, out_channels, kernel_heights, pad=0, stri=1, embed_dim=50, drop=0.2):\n",
        "        super().__init__()\n",
        "        self.conv1 = Conv2d(in_channels, out_channels[0], kernel_size=(kernel_heights[0], embed_dim), stride=stri, padding=pad)\n",
        "        self.conv2 = Conv2d(in_channels, out_channels[1], kernel_size=(kernel_heights[1], embed_dim), stride=stri, padding=pad)\n",
        "        self.conv3 = Conv2d(in_channels, out_channels[2], kernel_size=(kernel_heights[2], embed_dim), stride=stri, padding=pad)\n",
        "        self.drop = Dropout(drop)\n",
        "        self.fc = Linear(sum(out_channels), num_classes)\n",
        "        self.soft = Softmax(dim=1)\n",
        "\n",
        "    def _conv_n_maxpool_1d(self, input, conv_layer):\n",
        "\n",
        "        conved = conv_layer(input) # conved.size() = (batch_size, out_channels[0], dim, 1)\n",
        "        reld = F.relu(conved.squeeze(3)) # reld.size() = (batch_size, out_channels[0], dim)\n",
        "        max_out = F.max_pool1d(reld, reld.size()[2]).squeeze(2) # maxpool_out.size() = (batch_size, out_channels[0])\n",
        "\n",
        "        return max_out\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x.size() = (batch_size, num_seq, embed_dim)\n",
        "        x = x.unsqueeze(1) # x.size() = (batch_size, 1, num_seq, embed_dim)\n",
        "\n",
        "        out_1 = self._conv_n_maxpool_1d(x, self.conv1)\n",
        "        out_2 = self._conv_n_maxpool_1d(x, self.conv2)\n",
        "        out_3 = self._conv_n_maxpool_1d(x, self.conv3)\n",
        "\n",
        "        cat_out = torch.cat((out_1, out_2, out_3), dim=1)\n",
        "\n",
        "        drop = self.drop(cat_out)\n",
        "        fc_out = self.fc(drop)\n",
        "        out = self.soft(fc_out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "GQU3Gs6KLcdc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aHqK0WzS7_II"
      },
      "outputs": [],
      "source": [
        "class AdversarialClassificationDataset(Dataset):\n",
        "    def __init__(self, dataset_path, num_classes, tokenizer, model):\n",
        "        self.num_classes = num_classes\n",
        "        self.dataset = pd.read_csv(dataset_path)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.model = model\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        label = self.dataset[\"label\"][idx]\n",
        "        text = self.dataset[\"txt\"][idx]\n",
        "        \n",
        "        if self.model == 'BERT':\n",
        "            return label, self.tokenizer(text, padding=\"max_length\", return_tensors='pt', max_length=512, truncation=True)\n",
        "        else:\n",
        "            return label, self.tokenizer(text)\n",
        "\n",
        "class ClassificationDataset(Dataset):\n",
        "    def __init__(self, dataset, num_classes, tokenizer, model):\n",
        "        self.num_classes = num_classes\n",
        "        self.dataset = dataset\n",
        "        self.tokenizer = tokenizer\n",
        "        self.model = model\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.dataset.__len__()\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        label, text = self.dataset.__getitem__(idx)\n",
        "        if type(label) == str:\n",
        "            if label == 'neg':\n",
        "                label = 0\n",
        "            else:\n",
        "                label = 1\n",
        "        else:\n",
        "            label = int(label) - 1\n",
        "\n",
        "        if self.model == 'BERT':\n",
        "            return label, self.tokenizer(text, padding=\"max_length\", return_tensors='pt', max_length=512, truncation=True)\n",
        "        else:\n",
        "            return label, self.tokenizer(text)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "embedding = GloVe(name='6B', dim=50)\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list = [], []\n",
        "    for (_label, _tokens) in batch:\n",
        "        label_list.append(_label)\n",
        "        embed = embedding.get_vecs_by_tokens(_tokens)\n",
        "        text_list.append(embed)\n",
        "    text_list = pad_sequence(text_list, batch_first=True)\n",
        "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "    return label_list.to(device), text_list.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SELECT DATASET & MODEL\n",
        "\n",
        "DATASET_NAME = \"AG_NEWS_ADV.csv\"\n",
        "MODEL = \"LSTM\"\n",
        "DATASET = \"AG_NEWS\"\n",
        "num_classes = 4\n",
        "BATCH_SIZE = 64\n",
        "NUM_EPOCHS = 5\n",
        "SHUFFLE = True\n",
        "CLUSTER = False\n",
        "\n",
        "train_set = AdversarialClassificationDataset(PATH_DATASET+DATASET_NAME, num_classes, tokenizer, MODEL)\n",
        "\n",
        "test_set = AG_NEWS(split=\"test\")\n",
        "test_set = to_map_style_dataset(test_set)\n",
        "test_set = ClassificationDataset(test_set, num_classes, tokenizer, MODEL)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, collate_fn=collate_batch, shuffle=SHUFFLE)\n",
        "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, collate_fn=collate_batch, shuffle=SHUFFLE)"
      ],
      "metadata": {
        "id": "KPcTw4F1MJAX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, data_loader, loss=CrossEntropyLoss()):\n",
        "    model.eval()\n",
        "    total_acc, total_count = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        if MODEL == \"BERT\":\n",
        "            for idx, (labels, input_ids, token_type_ids, attention_mask) in enumerate(data_loader):\n",
        "                predicted_label = model(\n",
        "                    input_ids, token_type_ids, attention_mask)\n",
        "                loss_ = loss(predicted_label, labels)\n",
        "                total_acc += (predicted_label.argmax(1) == labels).sum().item()\n",
        "                total_count += labels.size(0)\n",
        "        else:\n",
        "            for idx, (labels, text) in enumerate(data_loader):\n",
        "                predicted_label = model(text)\n",
        "                loss_ = loss(predicted_label, labels)\n",
        "                total_acc += (predicted_label.argmax(1) == labels).sum().item()\n",
        "                total_count += labels.size(0)\n",
        "\n",
        "    return total_acc / total_count\n",
        "\n",
        "\n",
        "def train(model, optimizer, train_loader, loss=CrossEntropyLoss(), log_interval=50):\n",
        "    model.train()\n",
        "    total_acc, total_count = 0, 0\n",
        "    pbar = tqdm(total=len(train_loader),\n",
        "                desc=f'Epoch [{epoch + 1}/{NUM_EPOCHS}]')\n",
        "\n",
        "    if MODEL == 'BERT':\n",
        "        for idx, (labels, input_ids, token_type_ids, attention_mask) in enumerate(train_loader):\n",
        "            output = model(input_ids, token_type_ids, attention_mask)\n",
        "            loss_ = loss(output, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss_.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            total_acc += (output.argmax(1) == labels).sum().item()\n",
        "            total_count += labels.size(0)\n",
        "            if not CLUSTER:\n",
        "                pbar.update()\n",
        "            if idx % log_interval == 0 and idx > 0:\n",
        "                if not CLUSTER:\n",
        "                    pbar.set_postfix(\n",
        "                        loss=loss_, accuracy=total_acc / total_count)\n",
        "                total_acc, total_count = 0, 0\n",
        "\n",
        "        pbar.close()\n",
        "    else:\n",
        "        for idx, (labels, text) in enumerate(train_loader):\n",
        "            output = model(text)\n",
        "            loss_ = loss(output, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss_.backward()\n",
        "            optimizer.step()\n",
        "            total_acc += (output.argmax(1) == labels).sum().item()\n",
        "            total_count += labels.size(0)\n",
        "            if not CLUSTER:\n",
        "                pbar.update()\n",
        "            if idx % log_interval == 0 and idx > 0:\n",
        "                if not CLUSTER:\n",
        "                    pbar.set_postfix(\n",
        "                        loss=loss_, accuracy=total_acc / total_count)\n",
        "                total_acc, total_count = 0, 0\n",
        "\n",
        "        pbar.close()\n"
      ],
      "metadata": {
        "id": "YSIrjTy2O_8o"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BidirectionalLSTMClassifier(num_classes, 64, 1).to(device)\n",
        "optim = Adam(model.parameters())\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    train(model, optim, train_loader)\n",
        "    torch.save({'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optim.state_dict()}, PATH + MODEL + '_' + DATASET + '_' + 'PWWS' + '.pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmnwO2IwPK15",
        "outputId": "94ff31c9-c842-4a02-e3ed-3d00247a54b6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [1/5]: 100%|██████████| 1931/1931 [01:18<00:00, 24.54it/s, accuracy=0.882, loss=tensor(0.8606, device='cuda:0', grad_fn=<NllLossBackward0>)]\n",
            "Epoch [2/5]: 100%|██████████| 1931/1931 [01:15<00:00, 25.66it/s, accuracy=0.875, loss=tensor(0.9023, device='cuda:0', grad_fn=<NllLossBackward0>)]\n",
            "Epoch [3/5]: 100%|██████████| 1931/1931 [01:14<00:00, 26.08it/s, accuracy=0.893, loss=tensor(0.8511, device='cuda:0', grad_fn=<NllLossBackward0>)]\n",
            "Epoch [4/5]: 100%|██████████| 1931/1931 [01:14<00:00, 25.94it/s, accuracy=0.9, loss=tensor(0.8225, device='cuda:0', grad_fn=<NllLossBackward0>)]\n",
            "Epoch [5/5]: 100%|██████████| 1931/1931 [01:14<00:00, 25.94it/s, accuracy=0.905, loss=tensor(0.8233, device='cuda:0', grad_fn=<NllLossBackward0>)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_accuracy = evaluate(model, test_loader)\n",
        "print(f'Test accuracy: {test_accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ysD4pbTPpBg",
        "outputId": "ee69581a-e7d7-453e-dd26-81e6a7166262"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.9010526315789473\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7DWFGd9PZ9oE"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}
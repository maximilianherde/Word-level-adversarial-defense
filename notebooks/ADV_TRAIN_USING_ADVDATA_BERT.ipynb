{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ADV_TRAIN_USING_ADVDATA_BERT.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bb63e9dae56d4739b0ee603fb73f853b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ce7743962f334f5ca631b9ec77a83065",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_94d4b5a64a644be4857a3af51eff3802",
              "IPY_MODEL_b3b642f21f054e019b094900faf71cf7",
              "IPY_MODEL_c36b84d8bd534b238d8a9ed0ceca80f3"
            ]
          }
        },
        "ce7743962f334f5ca631b9ec77a83065": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "94d4b5a64a644be4857a3af51eff3802": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f825c91515fa491a9ef52dfc89bdea77",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_17d12e2a9f9b4676bf32a257a02a0910"
          }
        },
        "b3b642f21f054e019b094900faf71cf7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_864051c1087e41bab2da318aad450355",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 440473133,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440473133,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_01b40c205b824f249e35d2f8d33fbae8"
          }
        },
        "c36b84d8bd534b238d8a9ed0ceca80f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_71d203172fb04e0983f67ec0dbcbec17",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 420M/420M [00:13&lt;00:00, 33.5MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0be8de6a4c354c9db8489efa6fde659d"
          }
        },
        "f825c91515fa491a9ef52dfc89bdea77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "17d12e2a9f9b4676bf32a257a02a0910": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "864051c1087e41bab2da318aad450355": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "01b40c205b824f249e35d2f8d33fbae8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "71d203172fb04e0983f67ec0dbcbec17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0be8de6a4c354c9db8489efa6fde659d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# MOUNT G_DRIVE\n",
        "from pathlib import Path\n",
        "from IPython import get_ipython\n",
        "on_colab = 'google.colab' in str(get_ipython())\n",
        "\n",
        "if on_colab:\n",
        "  from google.colab import drive\n",
        "  drive.mount(\"/content/gdrive\")\n",
        "\n",
        "PATH =  \"/content/gdrive/My Drive/DeepLearning/MODELS/\" if on_colab else \"./\"\n",
        "PATH_DATASET = \"/content/gdrive/My Drive/DeepLearning/DATASETS/\" if on_colab else \"./\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CeAOqnjy-4xl",
        "outputId": "24cce39d-cdee-4fa4-db21-fd873551f2b3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MODELS"
      ],
      "metadata": {
        "id": "CuzOajo4Lq0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Linear, Softmax, Conv2d, Dropout\n",
        "import torch.nn as nn\n",
        "from torch.nn import LSTM, GRU, Linear, Softmax\n",
        "from torchtext.vocab import GloVe\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torch.optim import Adam\n",
        "from tqdm import tqdm\n",
        "from torchtext.datasets import IMDB, AG_NEWS, YahooAnswers\n",
        "from torchtext.data import to_map_style_dataset\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "\n",
        "class BidirectionalLSTMClassifier(torch.nn.Module):\n",
        "    def __init__(self, num_classes, hidden_size, num_layers):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.LSTM = LSTM(50, hidden_size, num_layers=num_layers,\n",
        "                         batch_first=True, bidirectional=True)\n",
        "        self.linear = Linear(2 * hidden_size, num_classes)\n",
        "        self.softmax = Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, (h_n, _) = self.LSTM(x)\n",
        "        h_forward = h_n[2 * self.num_layers - 2]\n",
        "        h_backward = h_n[2 * self.num_layers - 1]\n",
        "        y = self.linear(torch.cat((h_forward, h_backward), 1))\n",
        "        return self.softmax(y)\n",
        "\n",
        "\n",
        "class CNNClassifier(torch.nn.Module):\n",
        "    def __init__(self, num_classes, in_channels, out_channels, kernel_heights, pad=0, stri=1, embed_dim=50, drop=0.2):\n",
        "        super().__init__()\n",
        "        self.conv1 = Conv2d(in_channels, out_channels[0], kernel_size=(kernel_heights[0], embed_dim), stride=stri, padding=pad)\n",
        "        self.conv2 = Conv2d(in_channels, out_channels[1], kernel_size=(kernel_heights[1], embed_dim), stride=stri, padding=pad)\n",
        "        self.conv3 = Conv2d(in_channels, out_channels[2], kernel_size=(kernel_heights[2], embed_dim), stride=stri, padding=pad)\n",
        "        self.drop = Dropout(drop)\n",
        "        self.fc = Linear(sum(out_channels), num_classes)\n",
        "        self.soft = Softmax(dim=1)\n",
        "\n",
        "    def _conv_n_maxpool_1d(self, input, conv_layer):\n",
        "\n",
        "        conved = conv_layer(input) # conved.size() = (batch_size, out_channels[0], dim, 1)\n",
        "        reld = F.relu(conved.squeeze(3)) # reld.size() = (batch_size, out_channels[0], dim)\n",
        "        max_out = F.max_pool1d(reld, reld.size()[2]).squeeze(2) # maxpool_out.size() = (batch_size, out_channels[0])\n",
        "\n",
        "        return max_out\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x.size() = (batch_size, num_seq, embed_dim)\n",
        "        x = x.unsqueeze(1) # x.size() = (batch_size, 1, num_seq, embed_dim)\n",
        "\n",
        "        out_1 = self._conv_n_maxpool_1d(x, self.conv1)\n",
        "        out_2 = self._conv_n_maxpool_1d(x, self.conv2)\n",
        "        out_3 = self._conv_n_maxpool_1d(x, self.conv3)\n",
        "\n",
        "        cat_out = torch.cat((out_1, out_2, out_3), dim=1)\n",
        "\n",
        "        drop = self.drop(cat_out)\n",
        "        fc_out = self.fc(drop)\n",
        "        out = self.soft(fc_out)\n",
        "\n",
        "        return out\n",
        "\n",
        "def get_child(model, *arg):\n",
        "    res = model\n",
        "    for i in arg:\n",
        "        res = list(res.children())[i]\n",
        "    return res\n",
        "\n",
        "def freeze_model(model):\n",
        "    for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "            \n",
        "def unfreeze_model(model):\n",
        "    for param in model.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "def count_parameters(model, trainable_only = True):\n",
        "    if trainable_only:\n",
        "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    else:\n",
        "        return sum(p.numel() for p in model.parameters())\n",
        "\n",
        "def custom_freezer(model):\n",
        "    unfreeze_model(model)\n",
        "\n",
        "    ## freeze whole BertLayer\n",
        "    for c in model.children():\n",
        "        if str(c).startswith('Bert'):\n",
        "            freeze_model(c)\n",
        "            \n",
        "    ## unfreeze top 2 layer in BertEncoder\n",
        "    bert_encoder = get_child(model, 0, 1, 0)\n",
        "    for i in range(1, 3):\n",
        "        m = bert_encoder[-i] \n",
        "        unfreeze_model(m)\n",
        "        \n",
        "    ## unfreeze Pooling layer\n",
        "    bert_pooling = get_child(model, 0, 2)\n",
        "    unfreeze_model(bert_pooling)\n",
        "\n",
        "    print('Trainable parameters: {}'.format(count_parameters(model, True)))\n",
        "    return model\n",
        "\n",
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "\n",
        "        self.bert = BertForSequenceClassification.from_pretrained(\n",
        "            'bert-base-uncased', num_labels=num_classes)\n",
        "        self.bert = custom_freezer(self.bert)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids, attention_mask):\n",
        "        y = self.bert(input_ids, token_type_ids, attention_mask)\n",
        "        return self.softmax(y.logits)"
      ],
      "metadata": {
        "id": "GQU3Gs6KLcdc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "aHqK0WzS7_II"
      },
      "outputs": [],
      "source": [
        "class AdversarialClassificationDataset(Dataset):\n",
        "    def __init__(self, dataset_path, num_classes, tokenizer, model):\n",
        "        self.num_classes = num_classes\n",
        "        self.dataset = pd.read_csv(dataset_path)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.model = model\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        label = self.dataset[\"label\"][idx]\n",
        "        text = self.dataset[\"txt\"][idx]\n",
        "        \n",
        "        if self.model == 'BERT':\n",
        "            return label, self.tokenizer(text, padding=\"max_length\", return_tensors='pt', max_length=512, truncation=True)\n",
        "        else:\n",
        "            return label, self.tokenizer(text)\n",
        "\n",
        "class ClassificationDataset(Dataset):\n",
        "    def __init__(self, dataset, num_classes, tokenizer, model):\n",
        "        self.num_classes = num_classes\n",
        "        self.dataset = dataset\n",
        "        self.tokenizer = tokenizer\n",
        "        self.model = model\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.dataset.__len__()\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        label, text = self.dataset.__getitem__(idx)\n",
        "        if type(label) == str:\n",
        "            if label == 'neg':\n",
        "                label = 0\n",
        "            else:\n",
        "                label = 1\n",
        "        else:\n",
        "            label = int(label) - 1\n",
        "\n",
        "        if self.model == 'BERT':\n",
        "            return label, self.tokenizer(text, padding=\"max_length\", return_tensors='pt', max_length=512, truncation=True)\n",
        "        else:\n",
        "            return label, self.tokenizer(text)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#embedding = GloVe(name='6B', dim=50)\n",
        "#tokenizer = get_tokenizer(\"basic_english\")\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
        "\n",
        "'''def collate_batch(batch):\n",
        "    label_list, text_list = [], []\n",
        "    for (_label, _tokens) in batch:\n",
        "        label_list.append(_label)\n",
        "        embed = embedding.get_vecs_by_tokens(_tokens)\n",
        "        text_list.append(embed)\n",
        "    text_list = pad_sequence(text_list, batch_first=True)\n",
        "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "    return label_list.to(device), text_list.to(device)'''\n",
        "\n",
        "def collate_BERT(batch):\n",
        "    label_list, input_ids, token_type_ids, attention_mask = [], [], [], []\n",
        "    for (_label, _dic) in batch:\n",
        "        label_list.append(_label)\n",
        "        input_ids.append(_dic['input_ids'])\n",
        "        token_type_ids.append(_dic['token_type_ids'])\n",
        "        attention_mask.append(_dic['attention_mask'])\n",
        "    label_list = torch.tensor(label_list, dtype=torch.int64).to(device)\n",
        "    input_ids = torch.cat(input_ids, dim=0).to(device)\n",
        "    token_type_ids = torch.cat(token_type_ids, dim=0).to(device)\n",
        "    attention_mask = torch.cat(attention_mask, dim=0).to(device)\n",
        "    return label_list, input_ids, token_type_ids, attention_mask"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SELECT DATASET & MODEL\n",
        "\n",
        "DATASET_NAME = \"AG_NEWS_ADV.csv\"\n",
        "MODEL = \"BERT\"\n",
        "DATASET = \"AG_NEWS\"\n",
        "num_classes = 4\n",
        "BATCH_SIZE = 64\n",
        "NUM_EPOCHS = 5\n",
        "SHUFFLE = True\n",
        "CLUSTER = False\n",
        "\n",
        "train_set = AdversarialClassificationDataset(PATH_DATASET+DATASET_NAME, num_classes, tokenizer, MODEL)\n",
        "\n",
        "test_set = AG_NEWS(split=\"test\")\n",
        "test_set = to_map_style_dataset(test_set)\n",
        "test_set = ClassificationDataset(test_set, num_classes, tokenizer, MODEL)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, collate_fn=collate_BERT, shuffle=SHUFFLE)\n",
        "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, collate_fn=collate_BERT, shuffle=SHUFFLE)"
      ],
      "metadata": {
        "id": "KPcTw4F1MJAX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a841b914-991f-4523-c737-1cb2993f327b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.86MB [00:00, 19.3MB/s]                  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, data_loader, loss=CrossEntropyLoss()):\n",
        "    model.eval()\n",
        "    total_acc, total_count = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        if MODEL == \"BERT\":\n",
        "            for idx, (labels, input_ids, token_type_ids, attention_mask) in enumerate(data_loader):\n",
        "                predicted_label = model(\n",
        "                    input_ids, token_type_ids, attention_mask)\n",
        "                loss_ = loss(predicted_label, labels)\n",
        "                total_acc += (predicted_label.argmax(1) == labels).sum().item()\n",
        "                total_count += labels.size(0)\n",
        "        else:\n",
        "            for idx, (labels, text) in enumerate(data_loader):\n",
        "                predicted_label = model(text)\n",
        "                loss_ = loss(predicted_label, labels)\n",
        "                total_acc += (predicted_label.argmax(1) == labels).sum().item()\n",
        "                total_count += labels.size(0)\n",
        "\n",
        "    return total_acc / total_count\n",
        "\n",
        "\n",
        "def train(model, optimizer, train_loader, loss=CrossEntropyLoss(), log_interval=50):\n",
        "    model.train()\n",
        "    total_acc, total_count = 0, 0\n",
        "    pbar = tqdm(total=len(train_loader),\n",
        "                desc=f'Epoch [{epoch + 1}/{NUM_EPOCHS}]')\n",
        "\n",
        "    if MODEL == 'BERT':\n",
        "        for idx, (labels, input_ids, token_type_ids, attention_mask) in enumerate(train_loader):\n",
        "            output = model(input_ids, token_type_ids, attention_mask)\n",
        "            loss_ = loss(output, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss_.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            total_acc += (output.argmax(1) == labels).sum().item()\n",
        "            total_count += labels.size(0)\n",
        "            if not CLUSTER:\n",
        "                pbar.update()\n",
        "            if idx % log_interval == 0 and idx > 0:\n",
        "                if not CLUSTER:\n",
        "                    pbar.set_postfix(\n",
        "                        loss=loss_, accuracy=total_acc / total_count)\n",
        "                total_acc, total_count = 0, 0\n",
        "\n",
        "        pbar.close()\n",
        "    else:\n",
        "        for idx, (labels, text) in enumerate(train_loader):\n",
        "            output = model(text)\n",
        "            loss_ = loss(output, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss_.backward()\n",
        "            optimizer.step()\n",
        "            total_acc += (output.argmax(1) == labels).sum().item()\n",
        "            total_count += labels.size(0)\n",
        "            if not CLUSTER:\n",
        "                pbar.update()\n",
        "            if idx % log_interval == 0 and idx > 0:\n",
        "                if not CLUSTER:\n",
        "                    pbar.set_postfix(\n",
        "                        loss=loss_, accuracy=total_acc / total_count)\n",
        "                total_acc, total_count = 0, 0\n",
        "\n",
        "        pbar.close()\n"
      ],
      "metadata": {
        "id": "YSIrjTy2O_8o"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BERTClassifier(num_classes).to(device)\n",
        "#optim = Adam(model.parameters())\n",
        "NUM_EPOCHS = 3\n",
        "optim = AdamW(model.parameters(), lr=3e-5, correct_bias=False)\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    train(model, optim, train_loader)\n",
        "    torch.save({'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optim.state_dict()}, PATH + MODEL + '_' + DATASET + '_' + 'PWWS' + '.pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522,
          "referenced_widgets": [
            "bb63e9dae56d4739b0ee603fb73f853b",
            "ce7743962f334f5ca631b9ec77a83065",
            "94d4b5a64a644be4857a3af51eff3802",
            "b3b642f21f054e019b094900faf71cf7",
            "c36b84d8bd534b238d8a9ed0ceca80f3",
            "f825c91515fa491a9ef52dfc89bdea77",
            "17d12e2a9f9b4676bf32a257a02a0910",
            "864051c1087e41bab2da318aad450355",
            "01b40c205b824f249e35d2f8d33fbae8",
            "71d203172fb04e0983f67ec0dbcbec17",
            "0be8de6a4c354c9db8489efa6fde659d"
          ]
        },
        "id": "hmnwO2IwPK15",
        "outputId": "6cbe547a-00ea-48e2-db03-e0aa7f5ed322"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bb63e9dae56d4739b0ee603fb73f853b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable parameters: 14769412\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [1/3]:   1%|          | 22/1931 [02:08<3:06:39,  5.87s/it]"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-14d0d6e3b3a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moptim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdamW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     torch.save({'epoch': epoch,\n\u001b[1;32m      8\u001b[0m             \u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-5a1ddbc202dd>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, train_loader, loss, log_interval)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mtotal_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0mtotal_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mCLUSTER\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_accuracy = evaluate(model, test_loader)\n",
        "print(f'Test accuracy: {test_accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ysD4pbTPpBg",
        "outputId": "ee69581a-e7d7-453e-dd26-81e6a7166262"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.9010526315789473\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7DWFGd9PZ9oE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
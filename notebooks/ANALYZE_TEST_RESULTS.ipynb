{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ANALYZE_TEST_RESULTS.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#!pip install transformers"
      ],
      "metadata": {
        "id": "Xb4TiO78l0Bq"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET = \"YahooAnswers\"\n",
        "MODEL = \"LSTM\"\n",
        "DEFENSE = \"CLEAN\"\n",
        "BATCH_SIZE = 64\n",
        "SHUFFLE = True"
      ],
      "metadata": {
        "id": "OUMquqKhmgoK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SJXqHdRNkNQ3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchtext.datasets import IMDB, AG_NEWS, YahooAnswers\n",
        "from torchtext.vocab import GloVe\n",
        "from torchtext.data import to_map_style_dataset\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torch.nn import LSTM, GRU, Linear, Softmax, CrossEntropyLoss\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, random_split, Dataset\n",
        "from torch.optim import Adam\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Linear, Softmax, Conv2d, Dropout\n",
        "import torch.nn as nn\n",
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "from sklearn.metrics import roc_auc_score, f1_score\n",
        "from nltk.corpus import wordnet as wn\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from IPython import get_ipython\n",
        "on_colab = 'google.colab' in str(get_ipython())\n",
        "\n",
        "if on_colab:\n",
        "  from google.colab import drive\n",
        "  drive.mount(\"/content/gdrive\")\n",
        "\n",
        "PATH =  \"/content/gdrive/My Drive/DeepLearning/MODELS/\" if on_colab else \"./\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUMQdNiAkYZx",
        "outputId": "ce2292e8-eac3-4705-c00e-8f3deb0ba418"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "METRICS"
      ],
      "metadata": {
        "id": "XbuyWYM3lUer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stats(model, MODEL, data_loader, avg):\n",
        "    model.eval()\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    y_pred_arg = []\n",
        "    with torch.no_grad():\n",
        "        if MODEL == \"BERT\":\n",
        "            for idx, (labels, input_ids, token_type_ids, attention_mask) in enumerate(data_loader):\n",
        "                y_pred.append(model(input_ids, token_type_ids, attention_mask))\n",
        "                y_pred_arg.append(model(input_ids, token_type_ids, attention_mask).argmax(1))\n",
        "                y_true.append(labels)\n",
        "        else:\n",
        "            for idx, (labels, text) in enumerate(data_loader):\n",
        "                y_pred.append(model(text))\n",
        "                y_pred_arg.append(model(text).argmax(1))\n",
        "                y_true.append(labels)\n",
        "\n",
        "    y_pred_t = torch.vstack(y_pred).to(\"cpu\").numpy()\n",
        "    y_true_t = torch.hstack(y_true).to(\"cpu\").numpy()\n",
        "    y_pred_arg_t = torch.hstack(y_pred_arg).to(\"cpu\").numpy()\n",
        "    # for binary case only pass one prob. column\n",
        "    if y_pred_t.shape[1] < 3:\n",
        "        y_pred_t = y_pred_t[:, 1]\n",
        "\n",
        "    acc = (y_pred_arg_t == y_true_t).sum().item()/len(y_true_t)\n",
        "    roc_auc = roc_auc_score(y_true_t, y_pred_t, multi_class='ovr', average=avg)\n",
        "    f1 = f1_score(y_true_t, y_pred_arg_t, average=avg)\n",
        "    return acc, roc_auc, f1"
      ],
      "metadata": {
        "id": "Gdthtecrk52F"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MODELS"
      ],
      "metadata": {
        "id": "M5AHHnm9lV7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_child(model, *arg):\n",
        "    res = model\n",
        "    for i in arg:\n",
        "        res = list(res.children())[i]\n",
        "    return res\n",
        "\n",
        "def freeze_model(model):\n",
        "    for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "            \n",
        "def unfreeze_model(model):\n",
        "    for param in model.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "def count_parameters(model, trainable_only = True):\n",
        "    if trainable_only:\n",
        "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    else:\n",
        "        return sum(p.numel() for p in model.parameters())\n",
        "\n",
        "def custom_freezer(model):\n",
        "    unfreeze_model(model)\n",
        "\n",
        "    ## freeze whole BertLayer\n",
        "    for c in model.children():\n",
        "        if str(c).startswith('Bert'):\n",
        "            freeze_model(c)\n",
        "            \n",
        "    ## unfreeze top 2 layer in BertEncoder\n",
        "    bert_encoder = get_child(model, 0, 1, 0)\n",
        "    for i in range(1, 3):\n",
        "        m = bert_encoder[-i] \n",
        "        unfreeze_model(m)\n",
        "        \n",
        "    ## unfreeze Pooling layer\n",
        "    bert_pooling = get_child(model, 0, 2)\n",
        "    unfreeze_model(bert_pooling)\n",
        "\n",
        "    print('Trainable parameters: {}'.format(count_parameters(model, True)))\n",
        "    return model\n",
        "\n",
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "\n",
        "        self.bert = BertForSequenceClassification.from_pretrained(\n",
        "            'bert-base-uncased', num_labels=num_classes)\n",
        "        self.bert = custom_freezer(self.bert)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids, attention_mask):\n",
        "        y = self.bert(input_ids, token_type_ids, attention_mask)\n",
        "        return self.softmax(y.logits)\n",
        "\n",
        "class BidirectionalLSTMClassifier(torch.nn.Module):\n",
        "    def __init__(self, num_classes, hidden_size, num_layers):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.LSTM = LSTM(50, hidden_size, num_layers=num_layers,\n",
        "                         batch_first=True, bidirectional=True)\n",
        "        self.linear = Linear(2 * hidden_size, num_classes)\n",
        "        self.softmax = Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, (h_n, _) = self.LSTM(x)\n",
        "        h_forward = h_n[2 * self.num_layers - 2]\n",
        "        h_backward = h_n[2 * self.num_layers - 1]\n",
        "        y = self.linear(torch.cat((h_forward, h_backward), 1))\n",
        "        return self.softmax(y)\n",
        "\n",
        "class CNNClassifier(torch.nn.Module):\n",
        "    def __init__(self, num_classes, in_channels, out_channels, kernel_heights, pad=0, stri=1, embed_dim=50, drop=0.2):\n",
        "        super().__init__()\n",
        "        self.conv1 = Conv2d(in_channels, out_channels[0], kernel_size=(kernel_heights[0], embed_dim), stride=stri, padding=pad)\n",
        "        self.conv2 = Conv2d(in_channels, out_channels[1], kernel_size=(kernel_heights[1], embed_dim), stride=stri, padding=pad)\n",
        "        self.conv3 = Conv2d(in_channels, out_channels[2], kernel_size=(kernel_heights[2], embed_dim), stride=stri, padding=pad)\n",
        "        self.drop = Dropout(drop)\n",
        "        self.fc = Linear(sum(out_channels), num_classes)\n",
        "        self.soft = Softmax(dim=1)\n",
        "\n",
        "    def _conv_n_maxpool_1d(self, input, conv_layer):\n",
        "\n",
        "        conved = conv_layer(input) # conved.size() = (batch_size, out_channels[0], dim, 1)\n",
        "        reld = F.relu(conved.squeeze(3)) # reld.size() = (batch_size, out_channels[0], dim)\n",
        "        max_out = F.max_pool1d(reld, reld.size()[2]).squeeze(2) # maxpool_out.size() = (batch_size, out_channels[0])\n",
        "\n",
        "        return max_out\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x.size() = (batch_size, num_seq, embed_dim)\n",
        "        x = x.unsqueeze(1) # x.size() = (batch_size, 1, num_seq, embed_dim)\n",
        "\n",
        "        out_1 = self._conv_n_maxpool_1d(x, self.conv1)\n",
        "        out_2 = self._conv_n_maxpool_1d(x, self.conv2)\n",
        "        out_3 = self._conv_n_maxpool_1d(x, self.conv3)\n",
        "\n",
        "        cat_out = torch.cat((out_1, out_2, out_3), dim=1)\n",
        "\n",
        "        drop = self.drop(cat_out)\n",
        "        fc_out = self.fc(drop)\n",
        "        out = self.soft(fc_out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "sygUJCiClNQz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATASET & DATALOADER"
      ],
      "metadata": {
        "id": "QsUApc3Hl8pH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class ClassificationDataset(Dataset):\n",
        "    def __init__(self, dataset, num_classes, tokenizer, model):\n",
        "        self.num_classes = num_classes\n",
        "        self.dataset = dataset\n",
        "        self.tokenizer = tokenizer\n",
        "        self.model = model\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.dataset.__len__()\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        label, text = self.dataset.__getitem__(idx)\n",
        "        if type(label) == str:\n",
        "            if label == 'neg':\n",
        "                label = 0\n",
        "            else:\n",
        "                label = 1\n",
        "        else:\n",
        "            label = int(label) - 1\n",
        "\n",
        "        if self.model == 'BERT':\n",
        "            return label, self.tokenizer(text, padding=\"max_length\", return_tensors='pt', max_length=512, truncation=True)\n",
        "        else:\n",
        "            return label, self.tokenizer(text)"
      ],
      "metadata": {
        "id": "Nxxxrd6Llhbn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_batch(batch):\n",
        "    label_list, text_list = [], []\n",
        "    for (_label, _tokens) in batch:\n",
        "        label_list.append(_label)\n",
        "        embed = embedding.get_vecs_by_tokens(_tokens)\n",
        "        text_list.append(embed)\n",
        "    text_list = pad_sequence(text_list, batch_first=True)\n",
        "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "    return label_list.to(device), text_list.to(device)\n",
        "\n",
        "def collate_BERT(batch):\n",
        "    label_list, input_ids, token_type_ids, attention_mask = [], [], [], []\n",
        "    for (_label, _dic) in batch:\n",
        "        label_list.append(_label)\n",
        "        input_ids.append(_dic['input_ids'])\n",
        "        token_type_ids.append(_dic['token_type_ids'])\n",
        "        attention_mask.append(_dic['attention_mask'])\n",
        "    label_list = torch.tensor(label_list, dtype=torch.int64).to(device)\n",
        "    input_ids = torch.cat(input_ids, dim=0).to(device)\n",
        "    token_type_ids = torch.cat(token_type_ids, dim=0).to(device)\n",
        "    attention_mask = torch.cat(attention_mask, dim=0).to(device)\n",
        "    return label_list, input_ids, token_type_ids, attention_mask"
      ],
      "metadata": {
        "id": "dt-nrZqjmyyN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "if MODEL == 'BERT':\n",
        "    tokenizer = BertTokenizer.from_pretrained(\n",
        "        \"bert-base-uncased\", do_lower_case=True)\n",
        "else:\n",
        "    tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "embedding = GloVe(name='6B', dim=50)"
      ],
      "metadata": {
        "id": "FLizZ0Cwl_Ju"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if DATASET == 'IMDB':\n",
        "    test_set = IMDB(split='test')\n",
        "    num_classes = 2\n",
        "elif DATASET == 'AG_NEWS':\n",
        "    test_set = AG_NEWS(split='test')\n",
        "    num_classes = 4\n",
        "elif DATASET == 'YahooAnswers':\n",
        "    test_set = YahooAnswers(split='test')\n",
        "    num_classes = 10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1Yqna-UmTnw",
        "outputId": "e9202461-530e-4e9e-c107-94201a87350f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 319M/319M [00:06<00:00, 48.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_set = to_map_style_dataset(test_set)"
      ],
      "metadata": {
        "id": "ldg1mMWDn5ba"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if MODEL != \"BERT\":\n",
        "    test_set = ClassificationDataset(test_set, num_classes, tokenizer, MODEL)\n",
        "    test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, collate_fn=collate_batch, shuffle=SHUFFLE)\n",
        "else:\n",
        "    test_set = ClassificationDataset(test_set, num_classes, tokenizer, MODEL)\n",
        "    test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, collate_fn=collate_BERT, shuffle=SHUFFLE)"
      ],
      "metadata": {
        "id": "FITSPgKkmz8N"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LOAD MODEL TO BE ANALYSED\n",
        "if MODEL == \"LSTM\":\n",
        "    model = BidirectionalLSTMClassifier(num_classes, 64, 1).to(device)\n",
        "elif MODEL == \"CNN\":\n",
        "    model = CNNClassifier(num_classes, 1, [3, 5, 7], [2, 3, 4]).to(device)\n",
        "else:\n",
        "    model = BERTClassifier(num_classes).to(device)\n",
        "\n",
        "model_name = MODEL + \"_\" + DATASET + \"_\" + DEFENSE + \".pt\"\n",
        "checkpoint = torch.load(PATH + model_name)\n",
        "\n",
        "model.load_state_dict(checkpoint['model_state_dict'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_dH7AhUnprY",
        "outputId": "c43ce8ac-1f78-4427-b7ea-f0af91fe0505"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stats_ = stats(model, MODEL, test_loader, avg=\"weighted\")\n",
        "\n",
        "print(\"STATS FOR {i}, {j}, {k}:\".format(i=MODEL, j=DATASET, k=DEFENSE))\n",
        "print(\"ACCURACY: {}\".format(stats_[0]))\n",
        "print(\"AU-ROC: {}\".format(stats_[1]))\n",
        "print(\"F1: {}\".format(stats_[2]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GP3sxTL7o2Ud",
        "outputId": "481c4c4f-bff2-4498-e67d-c2811d741943"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STATS FOR LSTM, YahooAnswers, CLEAN:\n",
            "ACCURACY: 0.7092666666666667\n",
            "AU-ROC: 0.932857810339506\n",
            "F1: 0.7027195526738057\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "53TYRFQPqrFA"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}
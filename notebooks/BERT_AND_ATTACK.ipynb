{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "BiRLM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb1e2e07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df0e5549-1821-4c49-ed88-9a9e4511fa98"
      },
      "source": [
        "import torch\n",
        "from torchtext.datasets import IMDB, AG_NEWS, YahooAnswers\n",
        "from torchtext.vocab import GloVe\n",
        "from torchtext.data import to_map_style_dataset\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torch.nn import LSTM, GRU, Linear, Softmax, CrossEntropyLoss\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, random_split, Dataset\n",
        "from torch.optim import Adam\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "!pip install transformers\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW"
      ],
      "id": "fb1e2e07",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.12.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.2.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.49.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ad904ad2"
      },
      "source": [
        "DATASET = 'IMDB'\n",
        "MODEL = 'BERT'\n",
        "VALIDATION_SPLIT = 0.5 # of test data\n",
        "BATCH_SIZE = 64\n",
        "SHUFFLE = True\n",
        "NUM_EPOCHS = 5\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "id": "ad904ad2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nd1STifJoCO3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "5f451b11-9e94-4206-d236-7478b630146c"
      },
      "source": [
        "'''from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "PATH = '/content/drive/MyDrive/Checkpoints/model'\n",
        "!mkdir '/content/drive/MyDrive/Checkpoints/model'''"
      ],
      "id": "nd1STifJoCO3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"from google.colab import drive\\ndrive.mount('/content/drive')\\nPATH = '/content/drive/MyDrive/Checkpoints/model'\\n!mkdir '/content/drive/MyDrive/Checkpoints/model\""
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT FREEZER"
      ],
      "metadata": {
        "id": "71tvckJRIwCq"
      },
      "id": "71tvckJRIwCq"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_child(model, *arg):\n",
        "    res = model\n",
        "    for i in arg:\n",
        "        res = list(res.children())[i]\n",
        "    return res\n",
        "\n",
        "def freeze_model(model):\n",
        "    for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "            \n",
        "def unfreeze_model(model):\n",
        "    for param in model.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "def count_parameters(model, trainable_only = True):\n",
        "    if trainable_only:\n",
        "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    else:\n",
        "        return sum(p.numel() for p in model.parameters())\n",
        "\n",
        "def custom_freezer(model):\n",
        "    unfreeze_model(model)\n",
        "    #print('All parameters unfreezed: {}'.format(count_parameters(model)))\n",
        "\n",
        "    ## freeze whole BertLayer\n",
        "    for c in model.children():\n",
        "        if str(c).startswith('Bert'):\n",
        "            freeze_model(c)\n",
        "            \n",
        "    ## unfreeze top 2 layer in BertEncoder\n",
        "    bert_encoder = get_child(model, 0, 1, 0)\n",
        "    for i in range(1, 3):\n",
        "        m = bert_encoder[-i] \n",
        "        #print('Unfreezing: {}'.format(m))\n",
        "        unfreeze_model(m)\n",
        "        \n",
        "    ## unfreeze Pooling layer\n",
        "    bert_pooling = get_child(model, 0, 2)\n",
        "    unfreeze_model(bert_pooling)\n",
        "\n",
        "    print('Trainable parameters: {}'.format(count_parameters(model, True)))\n",
        "    return model"
      ],
      "metadata": {
        "id": "PkE-hb-rILMP"
      },
      "id": "PkE-hb-rILMP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1d344f05"
      },
      "source": [
        "class BidirectionalLSTMClassifier(torch.nn.Module):\n",
        "    def __init__(self, num_classes, hidden_size, num_layers):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.LSTM = LSTM(50, hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
        "        self.linear = Linear(2 * hidden_size, num_classes)\n",
        "        self.softmax = Softmax(dim=1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        _, (h_n, _) = self.LSTM(x)\n",
        "        h_forward = h_n[2 * self.num_layers - 2]\n",
        "        h_backward = h_n[2 * self.num_layers - 1]\n",
        "        y = self.linear(torch.cat((h_forward, h_backward), 1))\n",
        "        return self.softmax(y)\n",
        "    \n",
        "    \n",
        "class BidirectionalGRUClassifier(torch.nn.Module):\n",
        "    def __init__(self, num_classes, hidden_size, num_layers):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.GRU = GRU(50, hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
        "        self.linear = Linear(2 * hidden_size, num_classes)\n",
        "        self.softmax = Softmax(dim=1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        _, h_n = self.GRU(x)\n",
        "        h_forward = h_n[2 * self.num_layers - 2]\n",
        "        h_backward = h_n[2 * self.num_layers - 1]\n",
        "        y = self.linear(torch.cat((h_forward, h_backward), 1))\n",
        "        return self.softmax(y)\n",
        "\n",
        "from transformers import BertForSequenceClassification\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "\n",
        "        self.bert = BertForSequenceClassification.from_pretrained(\n",
        "            'bert-base-uncased', num_labels=num_classes)\n",
        "        self.bert = custom_freezer(self.bert)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids, attention_mask):\n",
        "        y = self.bert(input_ids, token_type_ids, attention_mask)\n",
        "        return self.softmax(y.logits)\n"
      ],
      "id": "1d344f05",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dkNIsx4rBhp"
      },
      "source": [
        "class ClassificationDataset(Dataset):\n",
        "    def __init__(self, dataset, num_classes, tokenizer, model):\n",
        "        self.num_classes = num_classes\n",
        "        self.dataset = dataset\n",
        "        self.tokenizer = tokenizer\n",
        "        self.model = model\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.dataset.__len__()\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        label, text = self.dataset.__getitem__(idx)\n",
        "        if type(label) == str:\n",
        "            if label == 'neg':\n",
        "                label = 0\n",
        "            else:\n",
        "                label = 1\n",
        "        else:\n",
        "            label = int(label) - 1\n",
        "\n",
        "        if self.model == 'BERT':\n",
        "            return label, self.tokenizer(text, padding=\"max_length\", return_tensors='pt', max_length=512, truncation=True)\n",
        "        else:\n",
        "            return label, self.tokenizer(text)"
      ],
      "id": "3dkNIsx4rBhp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d001818b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0111f904-cd0c-48b2-fdfc-70b5d52617b2"
      },
      "source": [
        "if DATASET == 'IMDB':\n",
        "    train_set = IMDB(split='train')\n",
        "    test_set = IMDB(split='test')\n",
        "    num_classes = 2\n",
        "elif DATASET == 'AG_NEWS':\n",
        "    train_set = AG_NEWS(split='train')\n",
        "    test_set = AG_NEWS(split='test')\n",
        "    num_classes = 4\n",
        "elif DATASET == 'YahooAnswers':\n",
        "    train_set = YahooAnswers(split='train')\n",
        "    test_set = YahooAnswers(split='test')\n",
        "    num_classes = 10\n",
        "else:\n",
        "    raise ValueError()\n",
        "\n",
        "if MODEL == 'BERT':\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "else:\n",
        "    tokenizer = get_tokenizer('basic_english')\n",
        "#embedding = GloVe(name='6B', dim=50)\n",
        "\n",
        "train_set = to_map_style_dataset(train_set)\n",
        "test_set = to_map_style_dataset(test_set)\n",
        "\n",
        "train_set = ClassificationDataset(train_set, num_classes, tokenizer, MODEL)\n",
        "test_set = ClassificationDataset(test_set, num_classes, tokenizer, MODEL)\n",
        "test_set, val_set = random_split(test_set, [test_set.__len__() - int(VALIDATION_SPLIT * test_set.__len__(\n",
        ")), int(VALIDATION_SPLIT * test_set.__len__())], generator=torch.Generator().manual_seed(42))"
      ],
      "id": "d001818b",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch [1/5]:   0%|          | 0/391 [02:37<?, ?it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xLC5SLAXPOp"
      },
      "source": [
        "# print(len(train_set.__getitem__(7)))"
      ],
      "id": "-xLC5SLAXPOp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4668a7f"
      },
      "source": [
        "def collate_batch(batch):\n",
        "    label_list, text_list = [], []\n",
        "    for (_label, _tokens) in batch:\n",
        "        label_list.append(_label)\n",
        "        embed = embedding.get_vecs_by_tokens(_tokens)\n",
        "        text_list.append(embed)\n",
        "    text_list = pad_sequence(text_list, batch_first=True)\n",
        "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "    return label_list.to(device), text_list.to(device)\n",
        "\n",
        "def collate_BERT(batch):\n",
        "    label_list, input_ids, token_type_ids, attention_mask = [], [], [], []\n",
        "    for (_label, _dic) in batch:\n",
        "        label_list.append(_label)\n",
        "        input_ids.append(_dic['input_ids'])\n",
        "        token_type_ids.append(_dic['token_type_ids'])\n",
        "        attention_mask.append(_dic['attention_mask'])\n",
        "    label_list = torch.tensor(label_list, dtype=torch.int64).to(device)\n",
        "    input_ids = torch.cat(input_ids, dim=0).to(device)\n",
        "    token_type_ids = torch.cat(token_type_ids, dim=0).to(device)\n",
        "    attention_mask = torch.cat(attention_mask, dim=0).to(device)\n",
        "    return label_list, input_ids, token_type_ids, attention_mask\n",
        "\n",
        "if MODEL == 'BERT':\n",
        "    train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, collate_fn=collate_BERT, shuffle=SHUFFLE)\n",
        "    test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, collate_fn=collate_BERT, shuffle=SHUFFLE)\n",
        "    val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, collate_fn=collate_BERT, shuffle=SHUFFLE)\n",
        "else:\n",
        "    train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, collate_fn=collate_batch, shuffle=SHUFFLE)\n",
        "    test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, collate_fn=collate_batch, shuffle=SHUFFLE)\n",
        "    val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, collate_fn=collate_batch, shuffle=SHUFFLE)"
      ],
      "id": "b4668a7f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eb76a3bd"
      },
      "source": [
        "def evaluate(model, data_loader, loss=CrossEntropyLoss()):\n",
        "    model.eval()\n",
        "    total_acc, total_count = 0, 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        if MODEL == \"BERT\":\n",
        "            for idx, (labels, input_ids, token_type_ids, attention_mask) in enumerate(data_loader):\n",
        "                predicted_label = model(input_ids, token_type_ids, attention_mask)\n",
        "                loss_ = loss(predicted_label, labels)\n",
        "                total_acc += (predicted_label.argmax(1) == labels).sum().item()\n",
        "                total_count += labels.size(0)\n",
        "        else:\n",
        "            for idx, (labels, text) in enumerate(data_loader):\n",
        "                predicted_label = model(text)\n",
        "                loss_ = loss(predicted_label, labels)\n",
        "                total_acc += (predicted_label.argmax(1) == labels).sum().item()\n",
        "                total_count += labels.size(0)\n",
        "    \n",
        "    return total_acc / total_count\n",
        "\n",
        "\n",
        "def train(model, optimizer, train_loader, loss=CrossEntropyLoss(), log_interval=50):\n",
        "    model.train()\n",
        "    total_acc, total_count = 0, 0\n",
        "    pbar = tqdm(total=len(train_loader), desc=f'Epoch [{epoch + 1}/{NUM_EPOCHS}]')\n",
        "\n",
        "    if MODEL == 'BERT':\n",
        "        for idx, (labels, input_ids, token_type_ids, attention_mask) in enumerate(train_loader):\n",
        "            output = model(input_ids, token_type_ids, attention_mask)\n",
        "            loss_ = loss(output, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss_.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            total_acc += (output.argmax(1) == labels).sum().item()\n",
        "            total_count += labels.size(0)\n",
        "            pbar.update()\n",
        "            if idx % log_interval == 0 and idx > 0:\n",
        "                pbar.set_postfix(loss=loss_, accuracy=total_acc / total_count)\n",
        "                total_acc, total_count = 0, 0\n",
        "        \n",
        "        pbar.close()\n",
        "    else:\n",
        "        for idx, (labels, text) in enumerate(train_loader):\n",
        "            output = model(text)\n",
        "            loss_ = loss(output, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss_.backward()\n",
        "            optimizer.step()\n",
        "            total_acc += (output.argmax(1) == labels).sum().item()\n",
        "            total_count += labels.size(0)\n",
        "            pbar.update()\n",
        "            if idx % log_interval == 0 and idx > 0:\n",
        "                pbar.set_postfix(loss=loss_, accuracy=total_acc / total_count)\n",
        "                total_acc, total_count = 0, 0\n",
        "        \n",
        "        pbar.close()"
      ],
      "id": "eb76a3bd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 932
        },
        "id": "e05c2e42",
        "outputId": "fa28047f-d774-4b8b-84ae-55eb3c4af5d1"
      },
      "source": [
        "model = BERTClassifier(num_classes).to(device)\n",
        "#optim = Adam(model.parameters())\n",
        "optim = AdamW(model.parameters(), lr=3e-5, correct_bias=False)\n",
        "\n",
        "for epoch in range(2):\n",
        "    train(model, optim, train_loader)\n",
        "    val_accuracy = evaluate(model, val_loader)\n",
        "    print(val_accuracy)\n",
        "\n",
        "    '''torch.save({\n",
        "        'epoch' : epoch,\n",
        "        'model_state_dict' : model.state_dict(),\n",
        "        'optimizer_state_dict': optim.state_dict(),\n",
        "        'val_accuracy' : val_accuracy\n",
        "    }, PATH + '_' + str(epoch) + '.pt' )'''\n",
        "\n",
        "    #How to load a model\n",
        "    #checkpoint = torch.load(PATH)\n",
        "    #model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    #optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    #epoch = checkpoint['epoch']\n",
        "    #val_accuracy = checkpoint['val_accuracy']"
      ],
      "id": "e05c2e42",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable parameters: 14767874\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch [1/5]:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch [1/5]:   0%|          | 1/391 [00:06<40:58,  6.30s/it]\u001b[A\n",
            "Epoch [1/5]:   1%|          | 2/391 [00:12<41:05,  6.34s/it]\u001b[A\n",
            "Epoch [1/5]:   1%|          | 3/391 [00:19<41:09,  6.36s/it]\u001b[A\n",
            "Epoch [1/5]:   1%|          | 4/391 [00:25<41:09,  6.38s/it]\u001b[A\n",
            "Epoch [1/5]:   1%|▏         | 5/391 [00:31<41:19,  6.42s/it]\u001b[A\n",
            "Epoch [1/5]:   2%|▏         | 6/391 [00:38<41:01,  6.39s/it]\u001b[A\n",
            "Epoch [1/5]:   2%|▏         | 7/391 [00:44<40:48,  6.38s/it]\u001b[A\n",
            "Epoch [1/5]:   2%|▏         | 8/391 [00:50<40:36,  6.36s/it]\u001b[A\n",
            "Epoch [1/5]:   2%|▏         | 9/391 [00:57<40:25,  6.35s/it]\u001b[A\n",
            "Epoch [1/5]:   3%|▎         | 10/391 [01:03<40:18,  6.35s/it]\u001b[A\n",
            "Epoch [1/5]:   3%|▎         | 11/391 [01:09<40:08,  6.34s/it]\u001b[A\n",
            "Epoch [1/5]:   3%|▎         | 12/391 [01:16<40:03,  6.34s/it]\u001b[A\n",
            "Epoch [1/5]:   3%|▎         | 13/391 [01:22<40:04,  6.36s/it]\u001b[A\n",
            "Epoch [1/5]:   4%|▎         | 14/391 [01:29<40:01,  6.37s/it]\u001b[A\n",
            "Epoch [1/5]:   4%|▍         | 15/391 [01:35<39:48,  6.35s/it]\u001b[A\n",
            "Epoch [1/5]:   4%|▍         | 16/391 [01:41<39:42,  6.35s/it]\u001b[A\n",
            "Epoch [1/5]:   4%|▍         | 17/391 [01:48<39:40,  6.36s/it]\u001b[A\n",
            "Epoch [1/5]:   5%|▍         | 18/391 [01:54<39:36,  6.37s/it]\u001b[A\n",
            "Epoch [1/5]:   5%|▍         | 19/391 [02:00<39:37,  6.39s/it]\u001b[A\n",
            "Epoch [1/5]:   5%|▌         | 20/391 [02:07<39:25,  6.38s/it]\u001b[A\n",
            "Epoch [1/5]:   5%|▌         | 21/391 [02:13<39:19,  6.38s/it]\u001b[A\n",
            "Epoch [1/5]:   6%|▌         | 22/391 [02:20<39:07,  6.36s/it]\u001b[A\n",
            "Epoch [1/5]:   6%|▌         | 23/391 [02:26<39:04,  6.37s/it]\u001b[A\n",
            "Epoch [1/5]:   6%|▌         | 24/391 [02:32<39:08,  6.40s/it]\u001b[A\n",
            "Epoch [1/5]:   6%|▋         | 25/391 [02:39<38:52,  6.37s/it]\u001b[A"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-3b2295f7d97c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mval_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-343461fae1f1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, train_loader, loss, log_interval)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mtotal_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0mtotal_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUlrEqCIFLuJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49087d83-3847-4266-8794-21aeca25792a"
      },
      "source": [
        "test_accuracy = evaluate(model, test_loader)\n",
        "print(f'Test accuracy: {test_accuracy}')"
      ],
      "id": "QUlrEqCIFLuJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.81512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install textattack[tensorflow]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtADdHYE7RU0",
        "outputId": "d911adf5-3b74-4b2f-f45c-f0294440276d"
      },
      "id": "BtADdHYE7RU0",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textattack[tensorflow] in /usr/local/lib/python3.7/dist-packages (0.3.4)\n",
            "Requirement already satisfied: lru-dict in /usr/local/lib/python3.7/dist-packages (from textattack[tensorflow]) (1.1.7)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from textattack[tensorflow]) (1.7.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from textattack[tensorflow]) (3.2.5)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.7/dist-packages (from textattack[tensorflow]) (0.5.3)\n",
            "Requirement already satisfied: terminaltables in /usr/local/lib/python3.7/dist-packages (from textattack[tensorflow]) (3.1.10)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from textattack[tensorflow]) (1.4.1)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from textattack[tensorflow]) (1.1.5)\n",
            "Requirement already satisfied: lemminflect in /usr/local/lib/python3.7/dist-packages (from textattack[tensorflow]) (0.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from textattack[tensorflow]) (3.4.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (from textattack[tensorflow]) (1.2.1)\n",
            "Requirement already satisfied: word2number in /usr/local/lib/python3.7/dist-packages (from textattack[tensorflow]) (1.1)\n",
            "Requirement already satisfied: numpy>=1.19.2 in /usr/local/lib/python3.7/dist-packages (from textattack[tensorflow]) (1.19.5)\n",
            "Requirement already satisfied: torch!=1.8,>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from textattack[tensorflow]) (1.10.0+cu111)\n",
            "Requirement already satisfied: language-tool-python in /usr/local/lib/python3.7/dist-packages (from textattack[tensorflow]) (2.6.2)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from textattack[tensorflow]) (4.49.0)\n",
            "Requirement already satisfied: bert-score>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from textattack[tensorflow]) (0.3.10)\n",
            "Requirement already satisfied: num2words in /usr/local/lib/python3.7/dist-packages (from textattack[tensorflow]) (0.5.10)\n",
            "Requirement already satisfied: flair in /usr/local/lib/python3.7/dist-packages (from textattack[tensorflow]) (0.10)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from textattack[tensorflow]) (8.8.0)\n",
            "Requirement already satisfied: transformers>=3.3.0 in /usr/local/lib/python3.7/dist-packages (from textattack[tensorflow]) (4.12.5)\n",
            "Requirement already satisfied: tensorflow-text>=2 in /usr/local/lib/python3.7/dist-packages (from textattack[tensorflow]) (2.5.0)\n",
            "Requirement already satisfied: tensorflow==2.5.0 in /usr/local/lib/python3.7/dist-packages (from textattack[tensorflow]) (2.5.0)\n",
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.7/dist-packages (from textattack[tensorflow]) (2.4.1)\n",
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.7/dist-packages (from textattack[tensorflow]) (0.12.0)\n",
            "Requirement already satisfied: tensorflow-estimator==2.5.0 in /usr/local/lib/python3.7/dist-packages (from textattack[tensorflow]) (2.5.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->textattack[tensorflow]) (0.12.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->textattack[tensorflow]) (0.2.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->textattack[tensorflow]) (1.12.1)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->textattack[tensorflow]) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->textattack[tensorflow]) (0.4.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->textattack[tensorflow]) (3.7.4.3)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->textattack[tensorflow]) (3.3.0)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->textattack[tensorflow]) (1.34.1)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->textattack[tensorflow]) (1.1.2)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->textattack[tensorflow]) (1.15.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->textattack[tensorflow]) (1.6.3)\n",
            "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->textattack[tensorflow]) (2.7.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->textattack[tensorflow]) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->textattack[tensorflow]) (3.17.3)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->textattack[tensorflow]) (1.12)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->textattack[tensorflow]) (0.37.0)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->textattack[tensorflow]) (3.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from bert-score>=0.3.5->textattack[tensorflow]) (2.23.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from bert-score>=0.3.5->textattack[tensorflow]) (3.2.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from bert-score>=0.3.5->textattack[tensorflow]) (21.3)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow==2.5.0->textattack[tensorflow]) (1.5.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->bert-score>=0.3.5->textattack[tensorflow]) (3.0.6)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.1->textattack[tensorflow]) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.1->textattack[tensorflow]) (2.8.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0->textattack[tensorflow]) (1.35.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0->textattack[tensorflow]) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0->textattack[tensorflow]) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0->textattack[tensorflow]) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0->textattack[tensorflow]) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0->textattack[tensorflow]) (1.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0->textattack[tensorflow]) (3.3.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0->textattack[tensorflow]) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0->textattack[tensorflow]) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0->textattack[tensorflow]) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow==2.5.0->textattack[tensorflow]) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow==2.5.0->textattack[tensorflow]) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.5->tensorflow==2.5.0->textattack[tensorflow]) (3.6.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0->textattack[tensorflow]) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->bert-score>=0.3.5->textattack[tensorflow]) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->bert-score>=0.3.5->textattack[tensorflow]) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->bert-score>=0.3.5->textattack[tensorflow]) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->bert-score>=0.3.5->textattack[tensorflow]) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow==2.5.0->textattack[tensorflow]) (3.1.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.3.0->textattack[tensorflow]) (6.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers>=3.3.0->textattack[tensorflow]) (0.0.46)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.3.0->textattack[tensorflow]) (0.10.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.3.0->textattack[tensorflow]) (2019.12.20)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.3.0->textattack[tensorflow]) (0.2.1)\n",
            "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from datasets->textattack[tensorflow]) (3.0.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets->textattack[tensorflow]) (0.70.12.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets->textattack[tensorflow]) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets->textattack[tensorflow]) (0.3.4)\n",
            "Requirement already satisfied: conllu>=4.0 in /usr/local/lib/python3.7/dist-packages (from flair->textattack[tensorflow]) (4.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from flair->textattack[tensorflow]) (1.0.1)\n",
            "Requirement already satisfied: janome in /usr/local/lib/python3.7/dist-packages (from flair->textattack[tensorflow]) (0.4.1)\n",
            "Requirement already satisfied: gdown==3.12.2 in /usr/local/lib/python3.7/dist-packages (from flair->textattack[tensorflow]) (3.12.2)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.7/dist-packages (from flair->textattack[tensorflow]) (1.0.9)\n",
            "Requirement already satisfied: bpemb>=0.3.2 in /usr/local/lib/python3.7/dist-packages (from flair->textattack[tensorflow]) (0.3.3)\n",
            "Requirement already satisfied: mpld3==0.3 in /usr/local/lib/python3.7/dist-packages (from flair->textattack[tensorflow]) (0.3)\n",
            "Requirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from flair->textattack[tensorflow]) (3.6.0)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from flair->textattack[tensorflow]) (6.0.3)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from flair->textattack[tensorflow]) (0.8.9)\n",
            "Requirement already satisfied: deprecated>=1.2.4 in /usr/local/lib/python3.7/dist-packages (from flair->textattack[tensorflow]) (1.2.13)\n",
            "Requirement already satisfied: sentencepiece==0.1.95 in /usr/local/lib/python3.7/dist-packages (from flair->textattack[tensorflow]) (0.1.95)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from flair->textattack[tensorflow]) (4.2.6)\n",
            "Requirement already satisfied: konoha<5.0.0,>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from flair->textattack[tensorflow]) (4.6.3)\n",
            "Requirement already satisfied: wikipedia-api in /usr/local/lib/python3.7/dist-packages (from flair->textattack[tensorflow]) (0.5.4)\n",
            "Requirement already satisfied: segtok>=1.5.7 in /usr/local/lib/python3.7/dist-packages (from flair->textattack[tensorflow]) (1.5.10)\n",
            "Requirement already satisfied: sqlitedict>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from flair->textattack[tensorflow]) (1.7.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim>=3.4.0->flair->textattack[tensorflow]) (5.2.1)\n",
            "Requirement already satisfied: overrides<4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from konoha<5.0.0,>=4.0.0->flair->textattack[tensorflow]) (3.1.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->bert-score>=0.3.5->textattack[tensorflow]) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->bert-score>=0.3.5->textattack[tensorflow]) (0.11.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->flair->textattack[tensorflow]) (3.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->flair->textattack[tensorflow]) (1.1.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->flair->textattack[tensorflow]) (0.2.5)\n",
            "Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.7/dist-packages (from num2words->textattack[tensorflow]) (0.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=3.3.0->textattack[tensorflow]) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5G30ec1aTgv"
      },
      "source": [
        "import textattack\n",
        "import torchtext\n",
        "import torch\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "class CustomPyTorchModelWrapper(textattack.models.wrappers.model_wrapper.ModelWrapper):\n",
        "    def __init__(self, model, outdim, vocab=torchtext.vocab.GloVe(\"6B\", dim=50), tokenizer=torchtext.data.utils.get_tokenizer(\"basic_english\")):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.outdim = outdim\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def __call__(self, text_input_list):\n",
        "        preds = torch.zeros(size=(len(text_input_list), self.outdim))\n",
        "        for i, review in enumerate(text_input_list):\n",
        "            tokens = self.tokenizer(review)\n",
        "            input = self.vocab.get_vecs_by_tokens(tokens)\n",
        "            with torch.no_grad():\n",
        "                prediction = self.model(torch.unsqueeze(input, dim=0))\n",
        "                preds[i] = prediction\n",
        "\n",
        "        return preds\n",
        "\n",
        "class CustomBERTModelWrapper(textattack.models.wrappers.model_wrapper.ModelWrapper):\n",
        "\n",
        "    def __init__(self, model, outdim, tokenizer=BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.outdim = outdim\n",
        "\n",
        "    \n",
        "    def __call__(self, text_input_list):\n",
        "        preds = torch.zeros(size=(len(text_input_list), self.outdim))\n",
        "        for i, review in enumerate(text_input_list):\n",
        "            dict_ = self.tokenizer(review, padding=\"max_length\", return_tensors='pt', max_length=512, truncation=True)\n",
        "            with torch.no_grad():\n",
        "                prediction = self.model(dict_[\"input_ids\"].to(device), dict_[\"token_type_ids\"].to(device), dict_[\"attention_mask\"].to(device))\n",
        "                preds[i] = prediction\n",
        "        \n",
        "        return preds    "
      ],
      "id": "f5G30ec1aTgv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BERTClassifier(num_classes).to(device)"
      ],
      "metadata": {
        "id": "rUOzWXD-7O-7"
      },
      "id": "rUOzWXD-7O-7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_wrapper = CustomBERTModelWrapper(model, outdim=num_classes)\n",
        "dataset = textattack.datasets.HuggingFaceDataset(\"imdb\", split=\"test\")\n",
        "attack = textattack.attack_recipes.pwws_ren_2019.PWWSRen2019.build(model_wrapper)\n",
        "attack_args = textattack.AttackArgs(num_examples=10)\n",
        "attacker = textattack.Attacker(attack, dataset, attack_args)\n",
        "attacker.attack_dataset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 871
        },
        "id": "o3Q0eo_g70QA",
        "outputId": "75f16692-acbe-44e3-98b5-99015fbd3fa2"
      },
      "id": "o3Q0eo_g70QA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3)\n",
            "textattack: Loading \u001b[94mdatasets\u001b[0m dataset \u001b[94mimdb\u001b[0m, split \u001b[94mtest\u001b[0m.\n",
            "textattack: Unknown if model of class <class '__main__.BERTClassifier'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  1%|          | 1/100 [00:00<00:10,  9.22it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "[Succeeded / Failed / Skipped / Total] 0 / 0 / 1 / 1:   1%|          | 1/100 [00:00<00:11,  8.39it/s]\u001b[A\u001b[A\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attack(\n",
            "  (search_method): GreedyWordSwapWIR(\n",
            "    (wir_method):  weighted-saliency\n",
            "  )\n",
            "  (goal_function):  UntargetedClassification\n",
            "  (transformation):  WordSwapWordNet\n",
            "  (constraints): \n",
            "    (0): RepeatModification\n",
            "    (1): StopwordModification\n",
            "  (is_black_box):  True\n",
            ") \n",
            "\n",
            "--------------------------------------------- Result 1 ---------------------------------------------\n",
            "[[Negative (51%)]] --> [[[SKIPPED]]]\n",
            "\n",
            "I went and saw this movie last night after being coaxed to by a few friends of mine. I'll admit that I was reluctant to see it because from what I knew of Ashton Kutcher he was only able to do comedy. I was wrong. Kutcher played the character of Jake Fischer very well, and Kevin Costner played Ben Randall with such professionalism. The sign of a good movie is that it can toy with our emotions. This one did exactly that. The entire theater (which was sold out) was overcome by laughter during the first half of the movie, and were moved to tears during the second half. While exiting the theater I not only saw many women in tears, but many full grown men as well, trying desperately not to let anyone see them crying. This movie was great, and I suggest that you go see it before you judge.\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-1e6c3f70f309>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mattack_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtextattack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttackArgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mattacker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtextattack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttacker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattack_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mattacker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattack_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/textattack/attacker.py\u001b[0m in \u001b[0;36mattack_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attack_parallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattack_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msilent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/textattack/attacker.py\u001b[0m in \u001b[0;36m_attack\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0mexample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattack_attrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mground_truth_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/textattack/attack.py\u001b[0m in \u001b[0;36mattack\u001b[0;34m(self, example, ground_truth_output)\u001b[0m\n\u001b[1;32m    420\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mSkippedAttackResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgoal_function_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgoal_function_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/textattack/attack.py\u001b[0m in \u001b[0;36m_attack\u001b[0;34m(self, initial_result)\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0;32mor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mMaximizedAttackResult\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \"\"\"\n\u001b[0;32m--> 371\u001b[0;31m         \u001b[0mfinal_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfinal_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgoal_status\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mGoalFunctionResultStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSUCCEEDED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/textattack/search_methods/search_method.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, initial_result)\u001b[0m\n\u001b[1;32m     34\u001b[0m             )\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperform_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0;31m# ensure that the number of queries for this GoalFunctionResult is up-to-date\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_queries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgoal_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_queries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/textattack/search_methods/greedy_word_swap_wir.py\u001b[0m in \u001b[0;36mperform_search\u001b[0;34m(self, initial_result)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformed_text_candidates\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearch_over\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_goal_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformed_text_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0;31m# Skip swaps which don't improve the score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/textattack/goal_functions/goal_function.py\u001b[0m in \u001b[0;36mget_results\u001b[0;34m(self, attacked_text_list, check_skip)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mattacked_text_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattacked_text_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mqueries_left\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_queries\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattacked_text_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattacked_text_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mattacked_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_output\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattacked_text_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mdisplayed_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_displayed_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/textattack/goal_functions/goal_function.py\u001b[0m in \u001b[0;36m_call_model\u001b[0;34m(self, attacked_text_list)\u001b[0m\n\u001b[1;32m    214\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_model_cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             ]\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_model_uncached\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muncached_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muncached_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_model_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/textattack/goal_functions/goal_function.py\u001b[0m in \u001b[0;36m_call_model_uncached\u001b[0;34m(self, attacked_text_list)\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0mbatch_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0;31m# Some seq-to-seq models will return a single string as a prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-6184efa90718>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text_input_list)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"token_type_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                 \u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}
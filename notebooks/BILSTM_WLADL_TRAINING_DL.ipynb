{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BILSTM_WLADL_TRAINING_DL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "82w9mnrMwOAe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchtext.datasets import IMDB, AG_NEWS, YahooAnswers\n",
        "from torchtext.vocab import GloVe\n",
        "from torchtext.data import to_map_style_dataset\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torch.nn import LSTM, GRU, Linear, Softmax, CrossEntropyLoss\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, random_split, Dataset\n",
        "from torch.optim import Adam\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET = 'IMDB'\n",
        "MODEL = 'LSTM'\n",
        "VALIDATION_SPLIT = 0.5 # of test data\n",
        "BATCH_SIZE = 64\n",
        "SHUFFLE = True\n",
        "NUM_EPOCHS = 5\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "lT7A3t6hwVdq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_thesaurus(all_words):\n",
        "    # Expects torch.vocab.itos dictionary to extract thesaurus\n",
        "    thesaurus = {}\n",
        "    syns = []\n",
        "    length_thesaurus = len(all_words)\n",
        "\n",
        "    for i in range(length_thesaurus):\n",
        "        if i % 10000 == 0:\n",
        "            print(\"At index {} of the vocabulary\".format(i))\n",
        "        # Extract the word\n",
        "        token = all_words[i]\n",
        "\n",
        "        # Find the synsets for the token\n",
        "        synsets = wn.synsets(token)\n",
        "\n",
        "        if len(synsets) == 0:\n",
        "            thesaurus[token] = \"\"\n",
        "        \n",
        "        else:\n",
        "            # Iterate through all synset\n",
        "            for synset in synsets:\n",
        "                lemma_names = synset.lemma_names()\n",
        "                for lemma in lemma_names:\n",
        "                    # Check if lemma has an underscore indicating a two word token\n",
        "                    if not(\"_\" in lemma):\n",
        "                        lemma = lemma.lower()\n",
        "                        if (lemma != token and lemma not in syns):\n",
        "                            syns.append(lemma)\n",
        "                    \n",
        "            \n",
        "            thesaurus[token] = syns\n",
        "            syns = []\n",
        "        \n",
        "    return thesaurus"
      ],
      "metadata": {
        "id": "ET27FhnV3AbG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BidirectionalLSTMClassifier(torch.nn.Module):\n",
        "    def __init__(self, num_classes, hidden_size, num_layers):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.LSTM = LSTM(50, hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
        "        self.linear = Linear(2 * hidden_size, num_classes)\n",
        "        self.softmax = Softmax(dim=1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        _, (h_n, _) = self.LSTM(x)\n",
        "        h_forward = h_n[2 * self.num_layers - 2]\n",
        "        h_backward = h_n[2 * self.num_layers - 1]\n",
        "        y = self.linear(torch.cat((h_forward, h_backward), 1))\n",
        "        return self.softmax(y)"
      ],
      "metadata": {
        "id": "seyhn2s-wZp4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassificationDataset(Dataset):\n",
        "    def __init__(self, dataset, num_classes, tokenizer, model):\n",
        "        self.num_classes = num_classes\n",
        "        self.dataset = dataset\n",
        "        self.tokenizer = tokenizer\n",
        "        self.model = model\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.dataset.__len__()\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        label, text = self.dataset.__getitem__(idx)\n",
        "        if type(label) == str:\n",
        "            if label == 'neg':\n",
        "                label = 0\n",
        "            else:\n",
        "                label = 1\n",
        "        else:\n",
        "            label = int(label) - 1\n",
        "\n",
        "        if self.model == 'BERT':\n",
        "            return label, self.tokenizer(text, padding=\"max_length\", return_tensors='pt', max_length=512, truncation=True)\n",
        "        else:\n",
        "            return label, self.tokenizer(text)"
      ],
      "metadata": {
        "id": "qhFFGA4owdVm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = IMDB(split='train')\n",
        "test_set = IMDB(split='test')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gI7cbok8wg9I",
        "outputId": "34997504-66c7-49fd-fbc2-e945fc4f9df6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 84.1M/84.1M [00:02<00:00, 39.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL = 'LSTM'\n",
        "num_classes = 2\n",
        "train_set = to_map_style_dataset(train_set)\n",
        "test_set = to_map_style_dataset(test_set)\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "train_set = ClassificationDataset(train_set, num_classes, tokenizer, MODEL)\n",
        "test_set = ClassificationDataset(test_set, num_classes, tokenizer, MODEL)"
      ],
      "metadata": {
        "id": "Av3T1xvRwlfV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = GloVe(name='6B', dim=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8De0EzMwoqL",
        "outputId": "03e89a35-0f1b-4786-eb3f-04d1e9f9f51b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:40, 5.37MB/s]                           \n",
            "100%|█████████▉| 399999/400000 [00:14<00:00, 27079.38it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "example_thes = build_thesaurus(embedding.itos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZCofjL53LUk",
        "outputId": "87ba87e7-11b2-4997-abe5-b798c1d72dc0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "At index 0 of the vocabulary\n",
            "At index 10000 of the vocabulary\n",
            "At index 20000 of the vocabulary\n",
            "At index 30000 of the vocabulary\n",
            "At index 40000 of the vocabulary\n",
            "At index 50000 of the vocabulary\n",
            "At index 60000 of the vocabulary\n",
            "At index 70000 of the vocabulary\n",
            "At index 80000 of the vocabulary\n",
            "At index 90000 of the vocabulary\n",
            "At index 100000 of the vocabulary\n",
            "At index 110000 of the vocabulary\n",
            "At index 120000 of the vocabulary\n",
            "At index 130000 of the vocabulary\n",
            "At index 140000 of the vocabulary\n",
            "At index 150000 of the vocabulary\n",
            "At index 160000 of the vocabulary\n",
            "At index 170000 of the vocabulary\n",
            "At index 180000 of the vocabulary\n",
            "At index 190000 of the vocabulary\n",
            "At index 200000 of the vocabulary\n",
            "At index 210000 of the vocabulary\n",
            "At index 220000 of the vocabulary\n",
            "At index 230000 of the vocabulary\n",
            "At index 240000 of the vocabulary\n",
            "At index 250000 of the vocabulary\n",
            "At index 260000 of the vocabulary\n",
            "At index 270000 of the vocabulary\n",
            "At index 280000 of the vocabulary\n",
            "At index 290000 of the vocabulary\n",
            "At index 300000 of the vocabulary\n",
            "At index 310000 of the vocabulary\n",
            "At index 320000 of the vocabulary\n",
            "At index 330000 of the vocabulary\n",
            "At index 340000 of the vocabulary\n",
            "At index 350000 of the vocabulary\n",
            "At index 360000 of the vocabulary\n",
            "At index 370000 of the vocabulary\n",
            "At index 380000 of the vocabulary\n",
            "At index 390000 of the vocabulary\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mask_replace_with_syns_add_noise(sentence, thesaurus, embedding, mask_probability=0.1, synonym_probability=0.25, pos_noise=0.1):\n",
        "    tokens_to_ret = []\n",
        "    for word in sentence:\n",
        "        mask_flag = np.random.choice([0, 1], replace=False, p=[1-mask_probability, mask_probability])\n",
        "        # Not masked\n",
        "        if mask_flag == 0:\n",
        "            syn_flag = np.random.choice([0, 1], replace=False, p=[1-synonym_probability, synonym_probability])\n",
        "            # Not masked & replaced with synonym\n",
        "            if syn_flag == 1:\n",
        "                # Check if word exists in thesaurus\n",
        "                synonyms = thesaurus.get(word)\n",
        "                if synonyms != None and len(synonyms) != 0:\n",
        "                    # randomly sample a synonym word\n",
        "                    indx = np.random.randint(low=0, high=len(synonyms))\n",
        "                    tokens_to_ret.append(synonyms[indx])\n",
        "                # Synonym doesn't exist\n",
        "                else:\n",
        "                    tokens_to_ret.append(word)\n",
        "            # Not masked & not replaced with synonym\n",
        "            else:\n",
        "                tokens_to_ret.append(word)\n",
        "        # Masked\n",
        "        else:\n",
        "            tokens_to_ret.append(\"\")\n",
        "        \n",
        "    \n",
        "    # We have masked and replaced with synonyms randomly, now obtain embeddings\n",
        "    embed = embedding.get_vecs_by_tokens(tokens_to_ret)\n",
        "    '''pos_encoding = np.zeros(embed.shape)\n",
        "    # Positional encoding introduced in Vaswani et. al.\n",
        "    for i in range(embed.shape[0]):\n",
        "        if i%2 == 0:\n",
        "            pos_param = pos_noise*np.sin(i / (10000 ** ((2*(i//2) / embed.shape[1]))))\n",
        "        else:\n",
        "            pos_param = pos_noise*np.cos(i / (10000 ** ((2*(i//2) / embed.shape[1]))))'''\n",
        "    return embed"
      ],
      "metadata": {
        "id": "WDGACAeI3Q2y"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_defence_batch(batch):\n",
        "    label_list, text_list = [], []\n",
        "    for (_label, _tokens) in batch:\n",
        "        label_list.append(_label)\n",
        "        #embed = embedding.get_vecs_by_tokens(_tokens)\n",
        "        embed = mask_replace_with_syns_add_noise(_tokens, example_thes, embedding)\n",
        "        text_list.append(embed)\n",
        "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "    text_list = pad_sequence(text_list, batch_first=True)\n",
        "    return label_list.to(device), text_list.to(device)"
      ],
      "metadata": {
        "id": "xW-BvsC83XSd"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_batch(batch):\n",
        "    label_list, text_list = [], []\n",
        "    for (_label, _tokens) in batch:\n",
        "        label_list.append(_label)\n",
        "        embed = embedding.get_vecs_by_tokens(_tokens)\n",
        "        text_list.append(embed)\n",
        "    text_list = pad_sequence(text_list, batch_first=True)\n",
        "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "    return label_list.to(device), text_list.to(device)"
      ],
      "metadata": {
        "id": "xxmSMtkEwx7L"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, collate_fn=collate_defence_batch, shuffle=SHUFFLE)\n",
        "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, collate_fn=collate_batch, shuffle=SHUFFLE)"
      ],
      "metadata": {
        "id": "ADeOxOXTw8D7"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, data_loader, loss=CrossEntropyLoss()):\n",
        "    model.eval()\n",
        "    total_acc, total_count = 0, 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        if MODEL == \"BERT\":\n",
        "            for idx, (labels, input_ids, token_type_ids, attention_mask) in enumerate(data_loader):\n",
        "                predicted_label = model(input_ids, token_type_ids, attention_mask)\n",
        "                loss_ = loss(predicted_label, labels)\n",
        "                total_acc += (predicted_label.argmax(1) == labels).sum().item()\n",
        "                total_count += labels.size(0)\n",
        "        else:\n",
        "            for idx, (labels, text) in enumerate(data_loader):\n",
        "                predicted_label = model(text)\n",
        "                loss_ = loss(predicted_label, labels)\n",
        "                total_acc += (predicted_label.argmax(1) == labels).sum().item()\n",
        "                total_count += labels.size(0)\n",
        "    \n",
        "    return total_acc / total_count\n",
        "\n",
        "\n",
        "def train(model, optimizer, train_loader, loss=CrossEntropyLoss(), log_interval=50):\n",
        "    model.train()\n",
        "    total_acc, total_count = 0, 0\n",
        "    pbar = tqdm(total=len(train_loader), desc=f'Epoch [{epoch + 1}/{NUM_EPOCHS}]')\n",
        "\n",
        "    if MODEL == 'BERT':\n",
        "        for idx, (labels, input_ids, token_type_ids, attention_mask) in enumerate(train_loader):\n",
        "            output = model(input_ids, token_type_ids, attention_mask)\n",
        "            loss_ = loss(output, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss_.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            total_acc += (output.argmax(1) == labels).sum().item()\n",
        "            total_count += labels.size(0)\n",
        "            pbar.update()\n",
        "            if idx % log_interval == 0 and idx > 0:\n",
        "                pbar.set_postfix(loss=loss_, accuracy=total_acc / total_count)\n",
        "                total_acc, total_count = 0, 0\n",
        "        \n",
        "        pbar.close()\n",
        "    else:\n",
        "        for idx, (labels, text) in enumerate(train_loader):\n",
        "            output = model(text)\n",
        "            loss_ = loss(output, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss_.backward()\n",
        "            optimizer.step()\n",
        "            total_acc += (output.argmax(1) == labels).sum().item()\n",
        "            total_count += labels.size(0)\n",
        "            pbar.update()\n",
        "            if idx % log_interval == 0 and idx > 0:\n",
        "                pbar.set_postfix(loss=loss_, accuracy=total_acc / total_count)\n",
        "                total_acc, total_count = 0, 0\n",
        "        \n",
        "        pbar.close()"
      ],
      "metadata": {
        "id": "dwsBZDGdw_im"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from IPython import get_ipython\n",
        "on_colab = 'google.colab' in str(get_ipython())\n",
        "\n",
        "if on_colab:\n",
        "  from google.colab import drive\n",
        "  drive.mount(\"/content/gdrive\")\n",
        "\n",
        "PATH =  \"/content/gdrive/My Drive/DeepLearning/MODELS/\" if on_colab else \"./\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njokHeBNxZLs",
        "outputId": "2818ce93-882f-4f05-e8e0-de359b9246ae"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = BidirectionalLSTMClassifier(num_classes, 64, 1).to(device)\n",
        "optim = Adam(model.parameters())\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    train(model, optim, train_loader)\n",
        "    torch.save({'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optim.state_dict()}, PATH + MODEL + '_' + DATASET + '_' + 'WLADL' + '.pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xQQBEqqxDyT",
        "outputId": "c5a987e2-89ec-4731-e3d8-3078940fe93e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [1/5]: 100%|██████████| 391/391 [18:21<00:00,  2.82s/it, accuracy=0.624, loss=tensor(0.6465, device='cuda:0', grad_fn=<NllLossBackward0>)]\n",
            "Epoch [2/5]: 100%|██████████| 391/391 [18:18<00:00,  2.81s/it, accuracy=0.512, loss=tensor(0.6899, device='cuda:0', grad_fn=<NllLossBackward0>)]\n",
            "Epoch [3/5]: 100%|██████████| 391/391 [18:26<00:00,  2.83s/it, accuracy=0.583, loss=tensor(0.6783, device='cuda:0', grad_fn=<NllLossBackward0>)]\n",
            "Epoch [4/5]: 100%|██████████| 391/391 [18:32<00:00,  2.85s/it, accuracy=0.768, loss=tensor(0.5120, device='cuda:0', grad_fn=<NllLossBackward0>)]\n",
            "Epoch [5/5]: 100%|██████████| 391/391 [18:27<00:00,  2.83s/it, accuracy=0.783, loss=tensor(0.5285, device='cuda:0', grad_fn=<NllLossBackward0>)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_accuracy = evaluate(model, test_loader)\n",
        "print(f'Test accuracy: {test_accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yITTtvkxhlU",
        "outputId": "22f05e30-a0d3-4505-9811-c62463af55ba"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.76912\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wDZlDGwozyxG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJbXRBMuvpVi",
        "outputId": "02ebfda1-a1aa-4a5a-e74e-7b0a22a7285f"
      },
      "id": "pJbXRBMuvpVi",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 21.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 57.1 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 49.6 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 56.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 432 kB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.2.1 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchtext.datasets import IMDB, AG_NEWS, YahooAnswers\n",
        "#from datasets_euler import AG_NEWS, IMDB, YahooAnswers\n",
        "from torchtext.data import to_map_style_dataset\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torch.nn import LSTM, GRU, Linear, Softmax, CrossEntropyLoss\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, random_split, Dataset\n",
        "from torch.optim import Adam\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "import math\n",
        "from scipy.spatial import distance\n",
        "import string\n",
        "from torchtext.vocab import GloVe, vocab\n",
        "import collections\n",
        "from collections import Counter, OrderedDict\n",
        "import pickle\n",
        "import itertools\n",
        "import sys\n",
        "import timeit\n",
        "import joblib\n",
        "\n",
        "DATASET = 'AG_NEWS'  # choose from IMDB, AG_NEWS, YahooAnswers\n",
        "MODEL = 'LSTM'  # choose from: GRU, LSTM, CNN, BERT, CNN2\n",
        "VALIDATION_SPLIT = 0.5  # of test data\n",
        "BATCH_SIZE = 64\n",
        "SHUFFLE = True\n",
        "NUM_EPOCHS = 5  # default 10\n",
        "VECTOR_CACHE = '/cluster/scratch/noec/'\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "PATH = '/content/drive/MyDrive/Checkpoints/School'\n",
        "filename = 'embedding.pt'\n",
        "embedding = torch.load(PATH + filename)\n",
        "\n",
        "class ClassificationDataset(Dataset):\n",
        "    def __init__(self, dataset, num_classes, tokenizer, model):\n",
        "        self.num_classes = num_classes\n",
        "        self.dataset = dataset\n",
        "        self.tokenizer = tokenizer\n",
        "        self.model = model\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.dataset.__len__()\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        label, text = self.dataset.__getitem__(idx)\n",
        "        if type(label) == str:\n",
        "            if label == 'neg':\n",
        "                label = 0\n",
        "            else:\n",
        "                label = 1\n",
        "        else:\n",
        "            label = int(label) - 1\n",
        "        if self.model == 'BERT':\n",
        "            return label, self.tokenizer(text, padding=\"max_length\", return_tensors='pt', max_length=512, truncation=True)\n",
        "        else:\n",
        "            return label, self.tokenizer(text)\n",
        "\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "if DATASET == 'IMDB':\n",
        "    train_set = IMDB(tokenizer, MODEL, split='train')\n",
        "    test_set = IMDB(tokenizer, MODEL, split='test')\n",
        "    num_classes = 2\n",
        "elif DATASET == 'AG_NEWS':\n",
        "    train_set = AG_NEWS(split='train')#(tokenizer, MODEL, split='train')\n",
        "    test_set = AG_NEWS(split='test')#(tokenizer, MODEL, split='test')\n",
        "    num_classes = 4\n",
        "elif DATASET == 'YahooAnswers':\n",
        "    train_set = YahooAnswers(tokenizer, MODEL, split='train')\n",
        "    test_set = YahooAnswers(tokenizer, MODEL, split='test')\n",
        "    num_classes = 10\n",
        "else:\n",
        "    raise ValueError()\n",
        "\n",
        "if MODEL == 'BERT':\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "else:\n",
        "    tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "train_set = to_map_style_dataset(train_set)\n",
        "test_set = to_map_style_dataset(test_set)\n",
        "\n",
        "train_set = ClassificationDataset(train_set, num_classes, tokenizer, MODEL)\n",
        "test_set = ClassificationDataset(test_set, num_classes, tokenizer, MODEL)\n",
        "test_set, val_set = random_split(test_set, [test_set.__len__() - int(VALIDATION_SPLIT * test_set.__len__(\n",
        ")), int(VALIDATION_SPLIT * test_set.__len__())], generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "def save_dictionary_to(PATH, dictionary_name, dictionary_to_save):\n",
        "  a_file = open(PATH + dictionary_name, 'wb')\n",
        "  joblib.dump(dictionary_to_save, a_file)\n",
        "  a_file.close()\n",
        "\n",
        "def return_dictionary_from(PATH, dictionary_name):\n",
        "  a_file = PATH + dictionary_name\n",
        "  output = joblib.load(a_file)\n",
        "  return output\n",
        "\n",
        "def save_torch_to(PATH, torch_name, torch_to_save):\n",
        "  savedest = PATH + torch_name\n",
        "  torch.save(torch_to_save, savedest)\n",
        "\n",
        "def return_torch_from(PATH, dictionary_name):\n",
        "  a_file = PATH + dictionary_name\n",
        "  output = torch.load(a_file)\n",
        "  return output\n",
        "\n",
        "def make_glove_counter():\n",
        "  PATH = '/content/drive/MyDrive/School/glove.6B/'\n",
        "  i = 0\n",
        "  glove_cnt = Counter()\n",
        "  with open(PATH + \"glove.6B.50d.txt\", 'r', encoding=\"utf-8\") as f:\n",
        "      for line in f:\n",
        "        i+=1\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        glove_cnt[word] = 0\n",
        "  return glove_cnt\n",
        "\n",
        "#returns a counter object with the frequency of the tokens present in list_with_tokens\n",
        "def make_frequency_counter(list_with_tokens):\n",
        "  return Counter(list_with_tokens)\n",
        "\n",
        "def print_first_n_key_val_dict(n, dictionary):\n",
        "  cnt = 0\n",
        "  for key, value in dictionary.items():\n",
        "    cnt += 1\n",
        "    print('Instance nr:', cnt)\n",
        "    print('key is:')\n",
        "    print(key)\n",
        "    print()\n",
        "    print('value is:')\n",
        "    print(value)\n",
        "    print()\n",
        "\n",
        "    if(cnt == n):\n",
        "      return\n",
        "\n",
        "def make_counter(train_set, test_set, val_set):\n",
        "  counter_dict = Counter()\n",
        "\n",
        "  for label, token_list in train_set:\n",
        "    new_dict = make_frequency_counter(token_list)\n",
        "    counter_dict.update(new_dict)\n",
        "\n",
        "  for label, token_list in test_set:\n",
        "    new_dict = make_frequency_counter(token_list)\n",
        "    counter_dict.update(new_dict)\n",
        "\n",
        "  for label, token_list in val_set:\n",
        "    new_dict = make_frequency_counter(token_list)\n",
        "    counter_dict.update(new_dict)\n",
        "\n",
        "  return counter_dict\n",
        "\n",
        "#returns the eucledian distance between 2 word embeddings\n",
        "def distance_between_tokens(token_1, token_2, embeddings):\n",
        "  return distance.euclidean(embeddings[token_1], embeddings[token_2])\n",
        "\n",
        "#returns a sorted collection object [word, eucl_dist] with respect to increasing eucl_dist\n",
        "#dict_with_tokens is a dictionary frequency_ordered\n",
        "def make_eucledian_distance_dict(token_reference, dict_with_tokens, embedding):\n",
        "  eucledian_distance = OrderedDict()\n",
        "  for token, frequency in dict_with_tokens.items():\n",
        "    eucledian_distance[token] = distance_between_tokens(token, token_reference, embedding)\n",
        "  return collections.OrderedDict(sorted(eucledian_distance.items(), key= lambda row: row[1]))\n",
        "\n",
        "#this returns a list with tuples. maximal size of the list is nr_of_synonyms\n",
        "# watch out that the dict_with_tokens has to be a dictionary element\n",
        "def possible_synonyms(token_ref, dict_with_tokens, distance_for_sin, nr_of_synonyms, embeddings):\n",
        "  dict_eucl_dist = make_eucledian_distance_dict(token_ref, dict_with_tokens, embedding)\n",
        "  counter = 0\n",
        "  synonyms = []\n",
        "  #code is constructed in such a manner that it starts from the most similar word\n",
        "  #which will always be itself\n",
        "  for token, eucledian_distance in dict_eucl_dist.items():\n",
        "    counter += 1\n",
        "    if(eucledian_distance <= distance_for_sin):\n",
        "      if (token != token_ref):\n",
        "        synonyms.append((token, eucledian_distance))\n",
        "    else:\n",
        "      break\n",
        "\n",
        "    if(counter >= (nr_of_synonyms+1)):\n",
        "      break\n",
        "\n",
        "  dict_eucl_dist.clear()\n",
        "  return synonyms\n",
        "\n",
        "\n",
        "#Out is a dictionary with the encoding results\n",
        "# I may assume the dict_of_words to be the frequency counter object sorted by frequency\n",
        "#encoding is a frequency ordered count dictionary \n",
        "def Synonym_Encoding_Algorithm(top_50000_dict, distance, nr_of_synonyms, \\\n",
        "                               embedding, glove_dict, return_tot_emb):\n",
        "\n",
        "  top_50000_dict = dict(top_50000_dict)\n",
        "\n",
        "  #Setting the value of the keys to NULL in order to flag them\n",
        "  for token, frequency in top_50000_dict.items():\n",
        "    top_50000_dict[token] = 'NULL'\n",
        "\n",
        "  counter = 0\n",
        "  tot_time = 0\n",
        "\n",
        "  print(\"Starting SEM\")\n",
        "  for token in top_50000_dict:\n",
        "    counter += 1\n",
        "    if(counter % 300 == 0):\n",
        "      print(counter,' out of ', len(top_50000_dict))\n",
        "      print('Expected total time is: ', 50000*tot_time/counter)\n",
        "\n",
        "    starttime = timeit.default_timer()\n",
        "    synonym_list = possible_synonyms(token, \\\n",
        "              top_50000_dict, distance, nr_of_synonyms, embedding)\n",
        "    endtime = timeit.default_timer()\n",
        "    tot_time += (endtime - starttime)\n",
        "\n",
        "    if (top_50000_dict[token] == 'NULL'):\n",
        "      #Looping through the synonyms\n",
        "      loop_counter = 0\n",
        "      for synonym_token, similarity in synonym_list:\n",
        "        loop_counter += 1\n",
        "\n",
        "        if (top_50000_dict[synonym_token] != 'NULL'):\n",
        "          loop_counter -= 1\n",
        "          top_50000_dict[token] = top_50000_dict[synonym_token]\n",
        "          break\n",
        "\n",
        "      #case if we didn't found encoded synonyms\n",
        "      if(loop_counter == len(synonym_list)):\n",
        "        top_50000_dict[token] = embedding[token]\n",
        "    synonym_list.clear()\n",
        "\n",
        "  #Creation of new_embedding dictionary with the hepl of embedding\n",
        "  if(return_tot_emb):\n",
        "    new_embedding = dict()\n",
        "    for token, _ in glove_dict.items():\n",
        "      new_embedding[token] = embedding[token]\n",
        "    new_embedding.update(top_50000_dict)\n",
        "    return new_embedding\n",
        "  else:\n",
        "    return top_50000_dict\n",
        "\n",
        "print('starting with functions')\n",
        "glove_dict = make_glove_counter()\n",
        "ds_cnt = make_counter(train_set, test_set, val_set)\n",
        "glove_dict.update(ds_cnt)\n",
        "top_n = 50000 #50000\n",
        "top_50000_list = glove_dict.most_common()[0:top_n]\n",
        "top_50000_dict = dict(top_50000_list)\n",
        "max_euclidian_distance = 3.6\n",
        "max_nr_of_synonyms = 10\n",
        "\n",
        "top_dict = make_frequency_counter(list_of_tks)\n",
        "\n",
        "print('starting with a new embedding')\n",
        "new_embedding = Synonym_Encoding_Algorithm(top_dict, \\\n",
        "  max_euclidian_distance, max_nr_of_synonyms, embedding, glove_dict, False)\n",
        "\n",
        "PATH = '/content/drive/MyDrive/School/'\n",
        "dictionary_name = 'new_embeddings_d_%.1f_k_%d.pt'%(max_euclidian_distance, max_nr_of_synonyms)\n",
        "\n",
        "torch.save(new_embedding, PATH + 'small_' +dictionary_name)\n",
        "\n",
        "print(\"New_Embeddings list saved in \" + PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0qBgMovtbYN",
        "outputId": "adfef0b5-175b-4fa4-8b3d-3c40fa5487e7"
      },
      "id": "L0qBgMovtbYN",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "29.5MB [00:00, 80.0MB/s]\n",
            "1.86MB [00:00, 73.9MB/s]                  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting with functions\n",
            "starting with a new embedding\n",
            "Time it took to upload the small dictionary: 1.689014190999984\n",
            "Embedding list saved in /content/drive/MyDrive/School/\n",
            "Time it took to upload the small dictionary: 13.24950096500001\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "SEM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
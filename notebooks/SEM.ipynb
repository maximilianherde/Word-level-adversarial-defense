{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b56de311",
      "metadata": {},
      "source": [
        "This script can be used on google colab, where it needs to be connected to a Drive with a repository which contains a directory called checkpoints which contains a directory called school."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pJbXRBMuvpVi",
      "metadata": {
        "id": "pJbXRBMuvpVi"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "L0qBgMovtbYN",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 560
        },
        "id": "L0qBgMovtbYN",
        "outputId": "0e2c606e-ca6a-4a3a-8133-f7a5fa5dfe7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "starting with functions\n",
            "starting with a new embedding\n",
            "Starting SEM\n",
            "10  out of  50000\n",
            "Expected total time is:  68704.99482000014\n",
            "20  out of  50000\n",
            "Expected total time is:  71953.62161000019\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-e10dbea9943e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'starting with a new embedding'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m \u001b[0mnew_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSynonym_Encoding_Algorithm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_50000_dict\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0mmax_euclidian_distance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_nr_of_synonyms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglove_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0mPATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/School/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-e10dbea9943e>\u001b[0m in \u001b[0;36mSynonym_Encoding_Algorithm\u001b[0;34m(top_50000_dict, distance, nr_of_synonyms, embedding, glove_dict, return_tot_emb)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0mstarttime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m     \u001b[0msynonym_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpossible_synonyms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m               \u001b[0mtop_50000_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnr_of_synonyms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m     \u001b[0mendtime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0mtot_time\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mendtime\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstarttime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-e10dbea9943e>\u001b[0m in \u001b[0;36mpossible_synonyms\u001b[0;34m(token_ref, dict_with_tokens, distance_for_sin, nr_of_synonyms, embeddings)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;31m# watch out that the dict_with_tokens has to be a dictionary element\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpossible_synonyms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_with_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistance_for_sin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnr_of_synonyms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m   \u001b[0mdict_eucl_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_eucledian_distance_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_with_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m   \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m   \u001b[0msynonyms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-e10dbea9943e>\u001b[0m in \u001b[0;36mmake_eucledian_distance_dict\u001b[0;34m(token_reference, dict_with_tokens, embedding)\u001b[0m\n\u001b[1;32m    169\u001b[0m   \u001b[0meucledian_distance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrequency\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdict_with_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0meucledian_distance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistance_between_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_reference\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meucledian_distance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-e10dbea9943e>\u001b[0m in \u001b[0;36mdistance_between_tokens\u001b[0;34m(token_1, token_2, embeddings)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;31m#returns the eucledian distance between 2 word embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdistance_between_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mdistance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meuclidean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken_1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken_2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;31m#returns a sorted collection object [word, eucl_dist] with respect to increasing eucl_dist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/spatial/distance.py\u001b[0m in \u001b[0;36meuclidean\u001b[0;34m(u, v, w)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \"\"\"\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mminkowski\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/spatial/distance.py\u001b[0m in \u001b[0;36mminkowski\u001b[0;34m(u, v, p, w)\u001b[0m\n\u001b[1;32m    507\u001b[0m     \"\"\"\n\u001b[1;32m    508\u001b[0m     \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"p must be at least 1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/spatial/distance.py\u001b[0m in \u001b[0;36m_validate_vector\u001b[0;34m(u, dtype)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_validate_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;31m# XXX Is order='c' really necessary?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m     \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m     \u001b[0;31m# Ensure values such as u=1 and u=[1] still return 1-D arrays.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torchtext.datasets import IMDB, AG_NEWS, YahooAnswers\n",
        "from torchtext.data import to_map_style_dataset\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torch.nn import LSTM, GRU, Linear, Softmax, CrossEntropyLoss\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, random_split, Dataset\n",
        "from torch.optim import Adam\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "import math\n",
        "from scipy.spatial import distance\n",
        "import string\n",
        "from torchtext.vocab import GloVe, vocab\n",
        "import collections\n",
        "from collections import Counter, OrderedDict\n",
        "import pickle\n",
        "import itertools\n",
        "import sys\n",
        "import timeit\n",
        "import joblib\n",
        "\n",
        "DATASET = 'AG_NEWS'  # choose from IMDB, AG_NEWS, YahooAnswers\n",
        "MODEL = 'LSTM'  # choose from: GRU, LSTM, CNN, BERT, CNN2\n",
        "VALIDATION_SPLIT = 0.5  # of test data\n",
        "BATCH_SIZE = 64\n",
        "SHUFFLE = True\n",
        "NUM_EPOCHS = 5  # default 10\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "PATH = '/content/drive/MyDrive/Checkpoints/School'\n",
        "filename = 'embedding.pt'\n",
        "embedding = torch.load(PATH + filename)\n",
        "\n",
        "# Class to handle the Dataset, it contains the tokenizer, the raw dataset and the\n",
        "# number of classes of the Dataset. It is useful especially if handling with document\n",
        "# classification tasks\n",
        "class ClassificationDataset(Dataset):\n",
        "    def __init__(self, dataset, num_classes, tokenizer, model):\n",
        "        self.num_classes = num_classes\n",
        "        self.dataset = dataset\n",
        "        self.tokenizer = tokenizer\n",
        "        self.model = model\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.dataset.__len__()\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        label, text = self.dataset.__getitem__(idx)\n",
        "        if type(label) == str:\n",
        "            if label == 'neg':\n",
        "                label = 0\n",
        "            else:\n",
        "                label = 1\n",
        "        else:\n",
        "            label = int(label) - 1\n",
        "        if self.model == 'BERT':\n",
        "            return label, self.tokenizer(text, padding=\"max_length\", return_tensors='pt', max_length=512, truncation=True)\n",
        "        else:\n",
        "            return label, self.tokenizer(text)\n",
        "\n",
        "if DATASET == 'IMDB':\n",
        "    train_set = IMDB(tokenizer, MODEL, split='train')\n",
        "    test_set = IMDB(tokenizer, MODEL, split='test')\n",
        "    num_classes = 2\n",
        "elif DATASET == 'AG_NEWS':\n",
        "    train_set = AG_NEWS(split='train')\n",
        "    test_set = AG_NEWS(split='test')\n",
        "    num_classes = 4\n",
        "elif DATASET == 'YahooAnswers':\n",
        "    train_set = YahooAnswers(tokenizer, MODEL, split='train')\n",
        "    test_set = YahooAnswers(tokenizer, MODEL, split='test')\n",
        "    num_classes = 10\n",
        "else:\n",
        "    raise ValueError()\n",
        "\n",
        "if MODEL == 'BERT':\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "else:\n",
        "    tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "train_set = to_map_style_dataset(train_set)\n",
        "test_set = to_map_style_dataset(test_set)\n",
        "\n",
        "train_set = ClassificationDataset(train_set, num_classes, tokenizer, MODEL)\n",
        "test_set = ClassificationDataset(test_set, num_classes, tokenizer, MODEL)\n",
        "test_set, val_set = random_split(test_set, [test_set.__len__() - int(VALIDATION_SPLIT * test_set.__len__(\n",
        ")), int(VALIDATION_SPLIT * test_set.__len__())], generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "# Here we need the glove.6B.50d.txt file in this repository /content/drive/MyDrive/School/glove.6B/\n",
        "# to work. The file can be downloaded at https://www.kaggle.com/watts2/glove6b50dtxt?select=glove.6B.50d.txt\n",
        "# The function returns a counter object which contains a dictionary of all tokens in GloVe.\n",
        "def make_glove_counter():\n",
        "  PATH = '/content/drive/MyDrive/School/glove.6B/'\n",
        "  i = 0\n",
        "  glove_cnt = Counter()\n",
        "  with open(PATH + \"glove.6B.50d.txt\", 'r', encoding=\"utf-8\") as f:\n",
        "      for line in f:\n",
        "        i+=1\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        glove_cnt[word] = 0\n",
        "  return glove_cnt\n",
        "\n",
        "# Returns a counter object with the frequency of the tokens present in list_with_tokens\n",
        "def make_frequency_counter(list_with_tokens):\n",
        "  return Counter(list_with_tokens)\n",
        "\n",
        "# Prints the first n keys in the dictionary\n",
        "def print_first_n_key_val_dict(n, dictionary):\n",
        "  cnt = 0\n",
        "  for key, value in dictionary.items():\n",
        "    cnt += 1\n",
        "    print('Instance nr:', cnt)\n",
        "    print('key is:')\n",
        "    print(key)\n",
        "    print()\n",
        "    print('value is:')\n",
        "    print(value)\n",
        "    print()\n",
        "\n",
        "    if(cnt == n):\n",
        "      return\n",
        "\n",
        "# Taking the train_set, the test_set and the val_set as we defined\n",
        "# them in our code and return a counter object dictionary which contains\n",
        "# the tokens and the number of times the token was used\n",
        "def make_counter(train_set, test_set, val_set):\n",
        "  counter_dict = Counter()\n",
        "\n",
        "  for label, token_list in train_set:\n",
        "    new_dict = make_frequency_counter(token_list)\n",
        "    counter_dict.update(new_dict)\n",
        "\n",
        "  for label, token_list in test_set:\n",
        "    new_dict = make_frequency_counter(token_list)\n",
        "    counter_dict.update(new_dict)\n",
        "\n",
        "  for label, token_list in val_set:\n",
        "    new_dict = make_frequency_counter(token_list)\n",
        "    counter_dict.update(new_dict)\n",
        "\n",
        "  return counter_dict\n",
        "\n",
        "# Returns the eucledian distance between 2 word embeddings\n",
        "def distance_between_tokens(token_1, token_2, embeddings):\n",
        "  return distance.euclidean(embeddings[token_1], embeddings[token_2])\n",
        "\n",
        "# Returns a sorted collection object [word, eucl_dist] with respect to increasing eucledian distance\n",
        "# to the token called token_reference.\n",
        "# We may assume that dict_with_tokens is a frequency ordered dictionary\n",
        "def make_eucledian_distance_dict(token_reference, dict_with_tokens, embedding):\n",
        "  eucledian_distance = OrderedDict()\n",
        "  for token, frequency in dict_with_tokens.items():\n",
        "    eucledian_distance[token] = distance_between_tokens(token, token_reference, embedding)\n",
        "  return collections.OrderedDict(sorted(eucledian_distance.items(), key= lambda row: row[1]))\n",
        "\n",
        "# Function returns a list with tuples. The maximal size of the list is nr_of_synonyms\n",
        "# watch out that the dict_with_tokens has to be a dictionary element\n",
        "def possible_synonyms(token_ref, dict_with_tokens, distance_for_sin, nr_of_synonyms, embeddings):\n",
        "  dict_eucl_dist = make_eucledian_distance_dict(token_ref, dict_with_tokens, embedding)\n",
        "  counter = 0\n",
        "  synonyms = []\n",
        "  # Loop is constructed in such a manner that it starts from the most similar word\n",
        "  # which will always be itself\n",
        "  for token, eucledian_distance in dict_eucl_dist.items():\n",
        "    counter += 1\n",
        "    if(eucledian_distance <= distance_for_sin):\n",
        "      if (token != token_ref):\n",
        "        synonyms.append((token, eucledian_distance))\n",
        "    else:\n",
        "      break\n",
        "\n",
        "    if(counter >= (nr_of_synonyms+1)):\n",
        "      break\n",
        "\n",
        "  dict_eucl_dist.clear()\n",
        "  return synonyms\n",
        "\n",
        "\n",
        "# Returns a dictionary with the new encodings according to the SEM Algorithm\n",
        "# I may assume the dict_of_words to be the frequency counter object sorted by frequency\n",
        "def Synonym_Encoding_Algorithm(top_nr_of_tks_dict, distance, nr_of_synonyms, \\\n",
        "                               embedding, glove_counter, return_tot_emb):\n",
        "\n",
        "  #Setting the value of the keys to NULL in order to flag them\n",
        "  for token, frequency in top_nr_of_tks_dict.items():\n",
        "    top_nr_of_tks_dict[token] = 'NULL'\n",
        "\n",
        "  counter = 0\n",
        "  tot_time = 0\n",
        "\n",
        "  print(\"Starting SEM\")\n",
        "  for token in top_nr_of_tks_dict:\n",
        "    counter += 1\n",
        "\n",
        "    # Self made function to look at progress while running\n",
        "    # On google colab in 12/2021 it was around 20 hours\n",
        "    if(counter % 1000 == 0):\n",
        "      print(counter,' out of ', len(top_nr_of_tks_dict))\n",
        "      print('Expected total time is: ', len(top_nr_of_tks_dict)*tot_time/counter)\n",
        "\n",
        "    starttime = timeit.default_timer()\n",
        "    synonym_list = possible_synonyms(token, \\\n",
        "              top_nr_of_tks_dict, distance, nr_of_synonyms, embedding)\n",
        "    endtime = timeit.default_timer()\n",
        "    tot_time += (endtime - starttime)\n",
        "\n",
        "    if (top_nr_of_tks_dict[token] == 'NULL'):\n",
        "      # Looping through the synonyms\n",
        "      loop_counter = 0\n",
        "      for synonym_token, similarity in synonym_list:\n",
        "        loop_counter += 1\n",
        "\n",
        "        if (top_nr_of_tks_dict[synonym_token] != 'NULL'):\n",
        "          loop_counter -= 1\n",
        "          top_nr_of_tks_dict[token] = top_nr_of_tks_dict[synonym_token]\n",
        "          break\n",
        "\n",
        "      # Case if we didn't found encoded synonyms\n",
        "      if(loop_counter == len(synonym_list)):\n",
        "        top_nr_of_tks_dict[token] = embedding[token]\n",
        "    synonym_list.clear()\n",
        "\n",
        "  # Here we incorporate the new embeddings in the old input embedding\n",
        "  # or we just return the embeddings of the top nr_of_tks words\n",
        "  if(return_tot_emb):\n",
        "    new_embedding = dict()\n",
        "    for token, _ in glove_counter.items():\n",
        "      new_embedding[token] = embedding[token]\n",
        "    new_embedding.update(top_nr_of_tks_dict)\n",
        "    return new_embedding\n",
        "  else:\n",
        "    return top_nr_of_tks_dict\n",
        "\n",
        "print('Preparing our counter objects')\n",
        "glove_counter = make_glove_counter()\n",
        "dataset_counter = make_counter(train_set, test_set, val_set)\n",
        "\n",
        "# We integrate the counter object from the dataset into our glove counter\n",
        "glove_counter.update(dataset_counter)\n",
        "\n",
        " # This variable defines the nr of words we input into SEM. It scales quadratically so beware\n",
        "nr_of_tks = 50000\n",
        "top_nr_of_tks_list = glove_counter.most_common()[0:nr_of_tks]\n",
        "top_nr_of_tks_dict = dict(top_nr_of_tks_list)\n",
        "\n",
        "# max_euclidian_distance determines how big the euclidian distance maximally is\n",
        "# between the embedding of 2 tokens in order to be considered similar.\n",
        "# The max_nr_of_synonyms determines the maximal number of synonyms a word can have\n",
        "max_euclidian_distance = 3.6\n",
        "max_nr_of_synonyms = 10\n",
        "\n",
        "print('Starting with a SEM')\n",
        "new_embedding = Synonym_Encoding_Algorithm(top_nr_of_tks_dict, \\\n",
        "  max_euclidian_distance, max_nr_of_synonyms, embedding, glove_counter, False)\n",
        "\n",
        "# Here we define the PATH were the new wmbwddings will be stored \n",
        "PATH = '/content/drive/MyDrive/School/'\n",
        "dictionary_name = 'new_embeddings_d_%.1f_k_%d.pt'%(max_euclidian_distance, max_nr_of_synonyms)\n",
        "\n",
        "torch.save(new_embedding, PATH + 'small_' +dictionary_name)\n",
        "\n",
        "print(\"New_Embeddings list saved in \" + PATH)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "SEM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "pJbXRBMuvpVi"
      },
      "id": "pJbXRBMuvpVi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchtext.datasets import IMDB, AG_NEWS, YahooAnswers\n",
        "#from datasets_euler import AG_NEWS, IMDB, YahooAnswers\n",
        "from torchtext.data import to_map_style_dataset\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torch.nn import LSTM, GRU, Linear, Softmax, CrossEntropyLoss\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, random_split, Dataset\n",
        "from torch.optim import Adam\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "import math\n",
        "from scipy.spatial import distance\n",
        "import string\n",
        "from torchtext.vocab import GloVe, vocab\n",
        "import collections\n",
        "from collections import Counter, OrderedDict\n",
        "import pickle\n",
        "import itertools\n",
        "import sys\n",
        "import timeit\n",
        "import joblib\n",
        "\n",
        "DATASET = 'AG_NEWS'  # choose from IMDB, AG_NEWS, YahooAnswers\n",
        "MODEL = 'LSTM'  # choose from: GRU, LSTM, CNN, BERT, CNN2\n",
        "VALIDATION_SPLIT = 0.5  # of test data\n",
        "BATCH_SIZE = 64\n",
        "SHUFFLE = True\n",
        "NUM_EPOCHS = 5  # default 10\n",
        "VECTOR_CACHE = '/cluster/scratch/noec/'\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "PATH = '/content/drive/MyDrive/Checkpoints/School'\n",
        "filename = 'embedding.pt'\n",
        "embedding = torch.load(PATH + filename)\n",
        "\n",
        "class ClassificationDataset(Dataset):\n",
        "    def __init__(self, dataset, num_classes, tokenizer, model):\n",
        "        self.num_classes = num_classes\n",
        "        self.dataset = dataset\n",
        "        self.tokenizer = tokenizer\n",
        "        self.model = model\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.dataset.__len__()\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        label, text = self.dataset.__getitem__(idx)\n",
        "        if type(label) == str:\n",
        "            if label == 'neg':\n",
        "                label = 0\n",
        "            else:\n",
        "                label = 1\n",
        "        else:\n",
        "            label = int(label) - 1\n",
        "        if self.model == 'BERT':\n",
        "            return label, self.tokenizer(text, padding=\"max_length\", return_tensors='pt', max_length=512, truncation=True)\n",
        "        else:\n",
        "            return label, self.tokenizer(text)\n",
        "\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "if DATASET == 'IMDB':\n",
        "    train_set = IMDB(tokenizer, MODEL, split='train')\n",
        "    test_set = IMDB(tokenizer, MODEL, split='test')\n",
        "    num_classes = 2\n",
        "elif DATASET == 'AG_NEWS':\n",
        "    train_set = AG_NEWS(split='train')#(tokenizer, MODEL, split='train')\n",
        "    test_set = AG_NEWS(split='test')#(tokenizer, MODEL, split='test')\n",
        "    num_classes = 4\n",
        "elif DATASET == 'YahooAnswers':\n",
        "    train_set = YahooAnswers(tokenizer, MODEL, split='train')\n",
        "    test_set = YahooAnswers(tokenizer, MODEL, split='test')\n",
        "    num_classes = 10\n",
        "else:\n",
        "    raise ValueError()\n",
        "\n",
        "if MODEL == 'BERT':\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "else:\n",
        "    tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "train_set = to_map_style_dataset(train_set)\n",
        "test_set = to_map_style_dataset(test_set)\n",
        "\n",
        "train_set = ClassificationDataset(train_set, num_classes, tokenizer, MODEL)\n",
        "test_set = ClassificationDataset(test_set, num_classes, tokenizer, MODEL)\n",
        "test_set, val_set = random_split(test_set, [test_set.__len__() - int(VALIDATION_SPLIT * test_set.__len__(\n",
        ")), int(VALIDATION_SPLIT * test_set.__len__())], generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "def save_dictionary_to(PATH, dictionary_name, dictionary_to_save):\n",
        "  a_file = open(PATH + dictionary_name, 'wb')\n",
        "  joblib.dump(dictionary_to_save, a_file)\n",
        "  a_file.close()\n",
        "\n",
        "def return_dictionary_from(PATH, dictionary_name):\n",
        "  a_file = PATH + dictionary_name\n",
        "  output = joblib.load(a_file)\n",
        "  return output\n",
        "\n",
        "def save_torch_to(PATH, torch_name, torch_to_save):\n",
        "  savedest = PATH + torch_name\n",
        "  torch.save(torch_to_save, savedest)\n",
        "\n",
        "def return_torch_from(PATH, dictionary_name):\n",
        "  a_file = PATH + dictionary_name\n",
        "  output = torch.load(a_file)\n",
        "  return output\n",
        "\n",
        "def make_glove_counter():\n",
        "  PATH = '/content/drive/MyDrive/School/glove.6B/'\n",
        "  i = 0\n",
        "  glove_cnt = Counter()\n",
        "  with open(PATH + \"glove.6B.50d.txt\", 'r', encoding=\"utf-8\") as f:\n",
        "      for line in f:\n",
        "        i+=1\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        glove_cnt[word] = 0\n",
        "  return glove_cnt\n",
        "\n",
        "#returns a counter object with the frequency of the tokens present in list_with_tokens\n",
        "def make_frequency_counter(list_with_tokens):\n",
        "  return Counter(list_with_tokens)\n",
        "\n",
        "def print_first_n_key_val_dict(n, dictionary):\n",
        "  cnt = 0\n",
        "  for key, value in dictionary.items():\n",
        "    cnt += 1\n",
        "    print('Instance nr:', cnt)\n",
        "    print('key is:')\n",
        "    print(key)\n",
        "    print()\n",
        "    print('value is:')\n",
        "    print(value)\n",
        "    print()\n",
        "\n",
        "    if(cnt == n):\n",
        "      return\n",
        "\n",
        "def make_counter(train_set, test_set, val_set):\n",
        "  counter_dict = Counter()\n",
        "\n",
        "  for label, token_list in train_set:\n",
        "    new_dict = make_frequency_counter(token_list)\n",
        "    counter_dict.update(new_dict)\n",
        "\n",
        "  for label, token_list in test_set:\n",
        "    new_dict = make_frequency_counter(token_list)\n",
        "    counter_dict.update(new_dict)\n",
        "\n",
        "  for label, token_list in val_set:\n",
        "    new_dict = make_frequency_counter(token_list)\n",
        "    counter_dict.update(new_dict)\n",
        "\n",
        "  return counter_dict\n",
        "\n",
        "#returns the eucledian distance between 2 word embeddings\n",
        "def distance_between_tokens(token_1, token_2, embeddings):\n",
        "  return distance.euclidean(embeddings[token_1], embeddings[token_2])\n",
        "\n",
        "#returns a sorted collection object [word, eucl_dist] with respect to increasing eucl_dist\n",
        "#dict_with_tokens is a dictionary frequency_ordered\n",
        "def make_eucledian_distance_dict(token_reference, dict_with_tokens, embedding):\n",
        "  eucledian_distance = OrderedDict()\n",
        "  for token, frequency in dict_with_tokens.items():\n",
        "    eucledian_distance[token] = distance_between_tokens(token, token_reference, embedding)\n",
        "  return collections.OrderedDict(sorted(eucledian_distance.items(), key= lambda row: row[1]))\n",
        "\n",
        "#this returns a list with tuples. maximal size of the list is nr_of_synonyms\n",
        "# watch out that the dict_with_tokens has to be a dictionary element\n",
        "def possible_synonyms(token_ref, dict_with_tokens, distance_for_sin, nr_of_synonyms, embeddings):\n",
        "  dict_eucl_dist = make_eucledian_distance_dict(token_ref, dict_with_tokens, embedding)\n",
        "  counter = 0\n",
        "  synonyms = []\n",
        "  #code is constructed in such a manner that it starts from the most similar word\n",
        "  #which will always be itself\n",
        "  for token, eucledian_distance in dict_eucl_dist.items():\n",
        "    counter += 1\n",
        "    if(eucledian_distance <= distance_for_sin):\n",
        "      if (token != token_ref):\n",
        "        synonyms.append((token, eucledian_distance))\n",
        "    else:\n",
        "      break\n",
        "\n",
        "    if(counter >= (nr_of_synonyms+1)):\n",
        "      break\n",
        "\n",
        "  dict_eucl_dist.clear()\n",
        "  return synonyms\n",
        "\n",
        "\n",
        "#Out is a dictionary with the encoding results\n",
        "# I may assume the dict_of_words to be the frequency counter object sorted by frequency\n",
        "#encoding is a frequency ordered count dictionary \n",
        "def Synonym_Encoding_Algorithm(top_50000_dict, distance, nr_of_synonyms, \\\n",
        "                               embedding, glove_dict, return_tot_emb):\n",
        "\n",
        "  #Setting the value of the keys to NULL in order to flag them\n",
        "  for token, frequency in top_50000_dict.items():\n",
        "    top_50000_dict[token] = 'NULL'\n",
        "\n",
        "  counter = 0\n",
        "  tot_time = 0\n",
        "\n",
        "  print(\"Starting SEM\")\n",
        "  for token in top_50000_dict:\n",
        "    counter += 1\n",
        "    if(counter % 10 == 0):\n",
        "      print(counter,' out of ', len(top_50000_dict))\n",
        "      print('Expected total time is: ', 50000*tot_time/counter)\n",
        "\n",
        "    starttime = timeit.default_timer()\n",
        "    synonym_list = possible_synonyms(token, \\\n",
        "              top_50000_dict, distance, nr_of_synonyms, embedding)\n",
        "    endtime = timeit.default_timer()\n",
        "    tot_time += (endtime - starttime)\n",
        "\n",
        "    if (top_50000_dict[token] == 'NULL'):\n",
        "      #Looping through the synonyms\n",
        "      loop_counter = 0\n",
        "      for synonym_token, similarity in synonym_list:\n",
        "        loop_counter += 1\n",
        "\n",
        "        if (top_50000_dict[synonym_token] != 'NULL'):\n",
        "          loop_counter -= 1\n",
        "          top_50000_dict[token] = top_50000_dict[synonym_token]\n",
        "          break\n",
        "\n",
        "      #case if we didn't found encoded synonyms\n",
        "      if(loop_counter == len(synonym_list)):\n",
        "        top_50000_dict[token] = embedding[token]\n",
        "    synonym_list.clear()\n",
        "\n",
        "  #Creation of new_embedding dictionary with the hepl of embedding\n",
        "  if(return_tot_emb):\n",
        "    new_embedding = dict()\n",
        "    for token, _ in glove_dict.items():\n",
        "      new_embedding[token] = embedding[token]\n",
        "    new_embedding.update(top_50000_dict)\n",
        "    return new_embedding\n",
        "  else:\n",
        "    return top_50000_dict\n",
        "\n",
        "print('starting with functions')\n",
        "glove_dict = make_glove_counter()\n",
        "ds_cnt = make_counter(train_set, test_set, val_set)\n",
        "glove_dict.update(ds_cnt)\n",
        "top_n = 50000 #50000\n",
        "top_50000_list = glove_dict.most_common()[0:top_n]\n",
        "top_50000_dict = dict(top_50000_list)\n",
        "max_euclidian_distance = 3.6\n",
        "max_nr_of_synonyms = 10\n",
        "\n",
        "print('starting with a new embedding')\n",
        "new_embedding = Synonym_Encoding_Algorithm(top_50000_dict, \\\n",
        "  max_euclidian_distance, max_nr_of_synonyms, embedding, glove_dict, False)\n",
        "\n",
        "PATH = '/content/drive/MyDrive/School/'\n",
        "dictionary_name = 'new_embeddings_d_%.1f_k_%d.pt'%(max_euclidian_distance, max_nr_of_synonyms)\n",
        "\n",
        "torch.save(new_embedding, PATH + 'small_' +dictionary_name)\n",
        "\n",
        "print(\"New_Embeddings list saved in \" + PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 560
        },
        "id": "L0qBgMovtbYN",
        "outputId": "0e2c606e-ca6a-4a3a-8133-f7a5fa5dfe7d"
      },
      "id": "L0qBgMovtbYN",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "starting with functions\n",
            "starting with a new embedding\n",
            "Starting SEM\n",
            "10  out of  50000\n",
            "Expected total time is:  68704.99482000014\n",
            "20  out of  50000\n",
            "Expected total time is:  71953.62161000019\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-e10dbea9943e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'starting with a new embedding'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m \u001b[0mnew_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSynonym_Encoding_Algorithm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_50000_dict\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0mmax_euclidian_distance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_nr_of_synonyms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglove_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0mPATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/School/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-e10dbea9943e>\u001b[0m in \u001b[0;36mSynonym_Encoding_Algorithm\u001b[0;34m(top_50000_dict, distance, nr_of_synonyms, embedding, glove_dict, return_tot_emb)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0mstarttime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m     \u001b[0msynonym_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpossible_synonyms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m               \u001b[0mtop_50000_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnr_of_synonyms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m     \u001b[0mendtime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0mtot_time\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mendtime\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstarttime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-e10dbea9943e>\u001b[0m in \u001b[0;36mpossible_synonyms\u001b[0;34m(token_ref, dict_with_tokens, distance_for_sin, nr_of_synonyms, embeddings)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;31m# watch out that the dict_with_tokens has to be a dictionary element\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpossible_synonyms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_with_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistance_for_sin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnr_of_synonyms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m   \u001b[0mdict_eucl_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_eucledian_distance_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_with_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m   \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m   \u001b[0msynonyms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-e10dbea9943e>\u001b[0m in \u001b[0;36mmake_eucledian_distance_dict\u001b[0;34m(token_reference, dict_with_tokens, embedding)\u001b[0m\n\u001b[1;32m    169\u001b[0m   \u001b[0meucledian_distance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrequency\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdict_with_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0meucledian_distance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistance_between_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_reference\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meucledian_distance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-e10dbea9943e>\u001b[0m in \u001b[0;36mdistance_between_tokens\u001b[0;34m(token_1, token_2, embeddings)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;31m#returns the eucledian distance between 2 word embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdistance_between_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mdistance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meuclidean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken_1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken_2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;31m#returns a sorted collection object [word, eucl_dist] with respect to increasing eucl_dist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/spatial/distance.py\u001b[0m in \u001b[0;36meuclidean\u001b[0;34m(u, v, w)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \"\"\"\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mminkowski\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/spatial/distance.py\u001b[0m in \u001b[0;36mminkowski\u001b[0;34m(u, v, p, w)\u001b[0m\n\u001b[1;32m    507\u001b[0m     \"\"\"\n\u001b[1;32m    508\u001b[0m     \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"p must be at least 1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/spatial/distance.py\u001b[0m in \u001b[0;36m_validate_vector\u001b[0;34m(u, dtype)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_validate_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;31m# XXX Is order='c' really necessary?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m     \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m     \u001b[0;31m# Ensure values such as u=1 and u=[1] still return 1-D arrays.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "SEM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}